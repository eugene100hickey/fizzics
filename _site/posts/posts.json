[
  {
    "path": "posts/2024-06-25-postcodes-arcgisgeocode/",
    "title": "Finding Postcodes using `arcgisgeocode`",
    "description": "Using the arcgiscoder package to find postal codes, making a shiny app using leaflet.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://www.fizzics.ie"
      }
    ],
    "date": "2024-06-25",
    "categories": [],
    "contents": "\r\nI’m old fashioned in a lot of ways, one of which is that I like to send postcards when on holidays. This means making sure they make their way to the intended recipients. And while I can usually remember an address, coming up with the corresponding postcode is at best hit-and-miss. There’s no telling how many postcards down the years are still out there drifting through mail-sorting limbo.\r\nThen I listened to a presentation by Josiah Parry and felt I had a solution to my problem. He introduced a package called arcgisgeocoder he’s been working on, part of the arcgis system. One of its capabilities is to take a geographic location and feed back details about the address, including the postal code. Kind of what I need. Josiah went on to incorporate this into a shiny app using leaflet, something I’m going to copy here. Then, when you scroll around the map and click on the location, it gives you the address and postal code.\r\nLet’s see how this works. First, we’ll need some packages:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(sf)\r\nlibrary(leaflet)\r\nlibrary(shiny)\r\nlibrary(bslib)\r\nlibrary(sf)\r\nlibrary(arcgisgeocode)\r\n\r\n\r\nThen we create our shiny app itself. Note the function reverse_geocode from arcgisgeocode. This does all the heavy lifting. The rest is fairly standard map interaction; displaying a map and capturing the latitude and longitude of a point clicked.\r\n\r\n\r\nui <- page_fillable(\r\n  card(\r\n    card_title(textOutput(\"rev_result\")),\r\n    leafletOutput(\"map\", width = \"100%\", height = \"100%\"),\r\n  )\r\n)\r\n\r\nserver <- function(input, output, session){\r\n  observeEvent(input$map_click, {\r\n    click <- input$map_click\r\n    x <- click$lng\r\n    y <- click$lat\r\n    loc <- c(x, y)\r\n    dput(loc)\r\n    \r\n    geocoded <- reverse_geocode(loc)\r\n    \r\n    output$rev_result <- renderText(\r\n      glue::glue(\"{geocoded$long_label}\\n{geocoded$postal}\")\r\n      )\r\n    \r\n    leafletProxy(\"map\", data = st_geometry(geocoded)) |> \r\n      clearMarkers() |> \r\n      addMarkers()\r\n  })\r\n  \r\n  output$map <- renderLeaflet({\r\n    leaflet() |> \r\n      addProviderTiles(providers$Esri.WorldGrayCanvas) |> \r\n      setView(lat = 53.32041, lng = -6.23956, zoom = 14)\r\n  })\r\n}\r\n\r\n\r\nshinyApp(ui, server)\r\n\r\n\r\nUnfortunately, shinyapps.io can’t handle the arcgiscoder package just yet, so I can’t deploy this app. Yet. But I’ll keep trying to find a way around this.\r\nAt the moment, shinyapps.io is having trouble working with arcgisgeocoder, otherwise I’d link to a web version of this app. I’ll work on this. For the moment, here is a version based on tidygeocoder that doesn’t quite work as well.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-06-25-postcodes-arcgisgeocode/images/sheephaven.png",
    "last_modified": "2024-07-17T23:26:10+01:00",
    "input_file": {},
    "preview_width": 1233,
    "preview_height": 956
  },
  {
    "path": "posts/2023-01-21-satellites-and-bedrock-iii/",
    "title": "Satellites and Bedrock - Part III",
    "description": "Looking at the ways in which satellite images from Sentinel could be used to examine underlying bedrock types. In this third of a three part series, we look at ways of stacking the satellite data and then squaring that off with what we've seen from bedrock.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://www.fizzics.ie"
      }
    ],
    "date": "2023-01-21",
    "categories": [
      "R",
      "Donegal",
      "Maps"
    ],
    "contents": "\r\nGeological Survey Ireland is producing wonderful data resources, impossible to resist when it comes to doing some analysis. In this post, welook at bedrock data, what you would see after sweeping away all the soil and loose covering from the ground. Bedrock is stuff like sandstone, limestone, igneous rock….\r\nI was interested to see how overlying vegetation depends on the bedrock; specifically if the colours from satellite images taken at different times of year could be used to take a guess at what lies beneath.\r\nThis requires two suites of information, the bedrock data from GSI and the satellite data from Copernicus-Sentinel. The first we’ll download directly and store locally from this resource, the second we’ll access using the R package, sen2r (but we’ll also end up downloading and storing image files locally, but sen2R will help scope them out).\r\nThis was originally written as one post, but then it got so long I decided to split it into three parts:\r\nPart I: getting the bedrock data from GSI\r\nPart II: getting the satellite data from Sentinel\r\nPart III: putting the first two together \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nA reminder, from part I, we accessed a map of bedrock data from GSI, this is ploted on the left below. From part II, we recruited three sets of satelllite images from Sentinel; one from June, one end-August, and the last from December 2022. Stacking these together gives us the middle plot. In addition, the right hand plot shows the August vegetation index NDVI, areas of darker green have more plant life.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe RGB colours from the raster image (in the center above) come from setting R to the NIR channel from June, G to the NIR channel of August, and B to the NIR channel of December. The red flecks over Donegal Bay correspond to clouds in the June image, the bue flecks to clouds in December (this is Ireland afterall)\r\nIt looks like there is some correspondence between bedrock and satellite here. The orange granite in the NorthWest quarter mapping to purple colours from the satellite stack, schist (brown) being lighter, and the arc of blue sandstone and limestone around Donegal Bay also apparent in the satellite photographs. None of this is too surprising; vegetation and growth on mountains will be different to that on plains. But it leads us on to our next step, can we determine the rock types based on what the satellite images are telling us? And using the red, green, blue bands as well as the near infra-red?\r\n\r\n\r\n\r\nNext we take the polygons defining areas in the bedrock map and use the exactextractr package to calculate the mean values of the satellite images in these areas. We then plot the NDVI values for these areas, colouring and circling them by rock type.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-21-satellites-and-bedrock-iii/satellites-and-bedrock-part-iii_files/figure-html5/ref-value-plot-1.png",
    "last_modified": "2023-01-30T16:58:55+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-01-19-satellites-and-bedrock-ii/",
    "title": "Satellites and Bedrock - Part II",
    "description": "Looking at the ways in which satellite images from Sentinel could be used to examine underlying bedrock types. In this second of a three part series, we look at ways of accessing satellite data from the Sentinel satellites.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://www.fizzics.ie"
      }
    ],
    "date": "2023-01-19",
    "categories": [
      "R",
      "Donegal",
      "Maps"
    ],
    "contents": "\r\nGeological Survey Ireland is producing wonderful data resources, impossible to resist when it comes to doing some analysis. In this post, welook at bedrock data, what you would see after sweeping away all the soil and loose covering from the ground. Bedrock is stuff like sandstone, limestone, igneous rock….\r\nI was interested to see how overlying vegetation depends on the bedrock; specifically if the colours from satellite images taken at different times of year could be used to take a guess at what lies beneath.\r\nThis requires two suites of information, the bedrock data from GSI and the satellite data from Copernicus-Sentinel. The first we’ll download directly and store locally from this resource, the second we’ll access using the R package, sen2r (but we’ll also end up downloading and storing image files locally, but sen2R will help scope them out).\r\nThis was originally written as one post, but then it got so long I decided to split it into three parts:\r\nPart I: getting the bedrock data from GSI\r\nPart II: getting the satellite data from Sentinel \r\nPart III: putting the first two together\r\n\r\n\r\n\r\nThis post is largely inspired by the work of Ewa Grabska and a WhyR presentation she made in 2020. She has a much clearer and in-depth view of the topic than I do.\r\nGetting access to Sentinel satellite data requires making an account at scihub. It’s free and open access. And once you have your login details you can do most of the leg work without leaving rstudio thanks to the sen2r and getSpatialData packages. As we’ll see, the only time we’ll leave rstudio is to unzip a downloaded file.\r\nThe first thing to do is to set an area of interest (aoi) using the set_aoi() function from getSpatialData. This pulls up a map of the globe in the rstudio viewer panel that can be zomed and panned to the geographical area of interest. When you’re there, click on the area select tool (black box on the left), place that on the map, and then click Done. This is shown below:\r\n\r\n\r\n\r\nYou can check that all has worked well using the getSpatialData::get_aoi() and the getSpatialData::view_aoi() functions.\r\nNext, we set the time frame of interest (early Summer is shown below), and the satellite platform de choix (Sentinel-2). And then we log in to Copernicus; a dialogue box will pop up asking you for your password.\r\n\r\n\r\ntime_range =  c(\"2022-05-03\", \"2022-07-28\") #set time range\r\nplatform = \"Sentinel-2\" #choose platform\r\nlogin_CopHub(\"eugene100hickey\")\r\n\r\n\r\n\r\n\r\n\r\nNow it’s time to see what Sentinel kows about this patch of Earth in this time period. The getSpatialData::getSentinelRecords() function is our friend here.\r\n\r\n\r\nquery <- getSentinel_records(time_range, \"Sentinel-2\")\r\n\r\n\r\nThis query is an sf object and is work looking at in detail. It could contains hundreds of individual records from Sentinel. It tells us about the size of each of the image file collections, what instrument on board the satellite was used, the precise time of measurement (always around mid-day, of course) as well as estimates of water coverage, snow and ice, and, importantly for images of Ireland, the amount of cloud cover.\r\nWe’ll filter on this last one, clouds may be beautiful, but they’re of no scientific interest here. In our case, this reduces the number of Sentinel fields from the hundreds down to a few tens.\r\n\r\n\r\nquery5 <- query |> filter(cloudcov < 5)\r\n\r\n\r\n\r\n\r\n\r\nSatellite\r\n      Date & Time\r\n      File Size\r\n      Cloud Cover (%)\r\n      Water (%)\r\n    Sentinel-2B\r\n2022-07-15 12:03:59\r\n58.49 MB\r\n3.3\r\n96.7Sentinel-2A\r\n2022-07-10 12:04:11\r\n41.95 MB\r\n0.0\r\nNASentinel-2B\r\n2022-06-29 11:43:59\r\n34.61 MB\r\n0.6\r\nNASentinel-2A\r\n2022-06-07 11:54:11\r\n221.81 MB\r\n3.5\r\nNASentinel-2B\r\n2022-06-05 12:03:59\r\n53.65 MB\r\n0.0\r\n100.0Sentinel-2A\r\n2022-06-04 11:44:01\r\n733.53 MB\r\n0.0\r\n82.1Sentinel-2A\r\n2022-05-15 11:43:51\r\n34.70 MB\r\n0.3\r\n99.7Sentinel-2B\r\n2022-05-06 12:03:59\r\n52.99 MB\r\n1.9\r\n98.1\r\n\r\nBefore downloading all this data, it’s good to have a sneak preview as to what they look like. To that end, we’ll use the getSpatialData::get_previews() function and then take a look with the getSpatialData::view_previews() function.\r\n\r\n\r\nrecords <- get_previews(query, dir_out = \"/home/some-directory/\")\r\nview_previews(records)\r\n# you can also see an individual record with, e.g. view_previews(records[1,])\r\n\r\n\r\nThis gives a mosaic of Sentinel captures, something like as seen below:\r\n\r\n\r\n\r\nWe then go and fetch the actual records (in this case the 10th one) we need with:\r\n\r\n\r\nset_archive(\"/home/some-directory/my-archive\")\r\ngetSentinel_data(records[10,])\r\n\r\n\r\nThis gives a zipped file. We unpack this where we want it to live. Later on, the files we care about will end up in a subfolder called GRANULE/Some-Sentinel-ID/IMG-DATA, where Some-Sentinel-ID is a long string made from a combination of image time, instrument, date…. This leads to a bunch of (mostly) jp2 files corresponding to different wavelength bands from blue to far infrared.\r\n\r\n\r\n\r\nFrom these image files, the only ones we’ll use will be the ******_B02.jp2 (red band), ******_B03.jp2 (green band), ******_B04.jp2 (blue band), and ******_B08.jp2 (IR band). We’ll see in the next installment how to stack these in to one image, and how to connect the colours back to our bedrock data.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-19-satellites-and-bedrock-ii/images/previews.png",
    "last_modified": "2023-01-19T18:19:05+00:00",
    "input_file": {},
    "preview_width": 545,
    "preview_height": 579
  },
  {
    "path": "posts/2023-01-17-satellites-and-bedrock/",
    "title": "Satellites and Bedrock - Part I",
    "description": "Looking at the ways in which satellite images from Sentinel could be used to examine underlying bedrock types. In this first of a three part series, we look at ways of accessing bedrock data from GSI.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://www.fizzics.ie"
      }
    ],
    "date": "2023-01-18",
    "categories": [
      "R",
      "Donegal",
      "Maps"
    ],
    "contents": "\r\n\r\n\r\nGeological Survey Ireland is producing wonderful data resources, impossible to resist when it comes to doing some analysis. In this post, welook at bedrock data, what you would see after sweeping away all the soil and loose covering from the ground. Bedrock is stuff like sandstone, limestone, igneous rock….\r\nI was interested to see how overlying vegetation depends on the bedrock; specifically if the colours from satellite images taken at different times of year could be used to take a guess at what lies beneath.\r\nThis requires two suites of information, the bedrock data from GSI and the satellite data from Copernicus-Sentinel. The first we’ll download directly and store locally from this resource, the second we’ll access using the R package, sen2r (but we’ll also end up downloading and storing image files locally, but sen2R will help scope them out).\r\nThis was originally written as one post, but then it got so long I decided to split it into three parts:\r\nPart I: getting the bedrock data from GSI \r\nPart II: getting the satellite data from Sentinel\r\nPart III: putting the first two together\r\n\r\n\r\n\r\n\r\n\r\nref <- st_read(\"data/Shapefiles/Bedrock_Polygons_ITM_2018.shp\", quiet = TRUE)\r\nmy_proj <- \"+proj=utm +zone=29 +datum=WGS84 +units=m +no_defs\"\r\nref <- st_transform(ref, crs = my_proj)\r\n\r\n\r\nThe bedrock file arrives in the form of a shapefile. It contains bunch of attributes: location, decription, area, as well as the polygon shapes themselves. And some attributes I’m at a loss to understand (M, Y, K, C anyone?). We end up with a total of 25,958 fields covering the Republic of Ireland.\r\nBelow we show the geometry for this shapefile:\r\n\r\nGeometry set for 25958 features \r\nGeometry type: MULTIPOLYGON\r\nDimension:     XY\r\nBounding box:  xmin: 384779.9 ymin: 5696646 xmax: 701597.5 ymax: 6145919\r\nCRS:           +proj=utm +zone=29 +datum=WGS84 +units=m +no_defs\r\nFirst 5 geometries:\r\n\r\nAnd here is a table showing the attributes for a sample of a few fields. Note, we’ve done a wee text-trawl through the description attribute to try and categorise the rocks in to a manageable number of types (quartz, sandstone, limestone, schist, shale, granite, or other).\r\n\r\n\r\nclass\r\n      NEWCODE\r\n      DESCRIPT\r\n      UNIT_NAME\r\n      M\r\n      Y\r\n      K\r\n      C\r\n      LABEL\r\n      SHAPE_AREA\r\n    limestone\r\nCDTOBE\r\nCalcareous shale, limestone conglomerate\r\nTober Colleen Formation\r\n20\r\n20\r\n0\r\n40\r\nTC\r\n1,943Kschist\r\nOTMAUL3\r\nDark grey semi-pelitic,  psammitic schist\r\nBallybeg Member\r\n15\r\n30\r\n0\r\n10\r\nMNbb\r\n821Kschist\r\nLCRQ\r\nPsammitic schist, some marble beds\r\nLower Crana Quartzite Formation\r\n30\r\n40\r\n0\r\n0\r\nLC\r\n41.17sandstone\r\nDUOHSF\r\nFlaser-bedded sandstone & minor mudstone\r\nOld Head Sandstone Formation\r\n0\r\n50\r\n0\r\n0\r\nOH\r\n1,766Klimestone\r\nCDRATH\r\nPale-grey massive mud-grade limestone\r\nRathronan Formation\r\n10\r\n0\r\n0\r\n20\r\nRR\r\n249Kquartz\r\nMRBENQ\r\nPale quartzites, grits, graphitic top\r\nBennabeola Quartzite Formation\r\n0\r\n70\r\n0\r\n0\r\nBX\r\n119.39other\r\nSMBRDF\r\nFine to conglomeratic graded greywacke\r\nBroadford Formation\r\n10\r\n50\r\n0\r\n30\r\nBF\r\n11Ksandstone\r\nS*GCRA\r\nThin-bedded sandstone & mudstone\r\nGlencraff Formation\r\n10\r\n30\r\n0\r\n10\r\nGC\r\n58Kother\r\nIOMGAB\r\nHornblende metagabbros & metanorites\r\nMetagabbro and Related Lithologies\r\n0\r\n40\r\n0\r\n70\r\nMg\r\n33.45sandstone\r\nDUCARR\r\nRed,  brown conglomerate & sandstone\r\nCarrigmaclea Formation\r\n40\r\n70\r\n0\r\n30\r\nCI\r\n1,364K\r\n\r\n\r\n\r\n\r\nPlotting this, and just looking at the NorthWest, we get:\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-01-17-satellites-and-bedrock/satellites-and-bedrock_files/figure-html5/map-1.png",
    "last_modified": "2023-01-22T13:44:42+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-08-14-brain-areas/",
    "title": "Brain Areas",
    "description": "Looking at the heirarchy of brain areas in the Allen Brain Atlas",
    "author": [
      {
        "name": "Eugene",
        "url": "https://www.fizzics.ie"
      }
    ],
    "date": "2022-08-14",
    "categories": [
      "R",
      "Bioinformatics",
      "Academia"
    ],
    "contents": "\r\nWe do a lot of work with data from the Allen Brain Atlas. Frequently,\r\ntheir data is broken down by brain area, with several hundred parts\r\nmaking up our brains. While a map helps a lot to find out where\r\neverything goes, I also like to know which areas are substructures of\r\nwhich other areas. It’s always nice to know that the trigeminal\r\nnucleus is one part of the Pons for example. So to that\r\nend, I built a hierarchy of brain areas in the form of a collapsible\r\ntree. I’m still working on preventing overlapping labels, but for the\r\nmost part is legible, just about, with the right level of zoom.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNext step is to map all these to the beautiful brain atlases produced\r\nby Athanasia Mowinckel with the\r\nggseg R package.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-14-brain-areas/images/brain.jpg",
    "last_modified": "2023-01-17T14:13:35+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-05-golf-in-donegal-ii/",
    "title": "Golf Courses in Donegal - Part II, analysis",
    "description": "A quick look at the pars, lengths, and handicap indices of golf courses in Donegal and surrounds. In this second part we'll look at some plots that investigating these numbers.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://www.fizzics.ie"
      }
    ],
    "date": "2022-08-05",
    "categories": [
      "Sport",
      "Donegal"
    ],
    "contents": "\r\nIn a previous post, we looked at the course cards for various golf clubs in Donegal and beyond. In this post, we aim to see what details we can determine from these cards about the length, difficulty and pars on these holes.\r\nThese are the libraries we’ll need, and also a ggplot theme I like to use:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(showtext)\r\nlibrary(ggokabeito)\r\nlibrary(viridis)\r\nlibrary(geomtextpath)\r\nlibrary(ggtext)\r\nlibrary(patchwork)\r\n\r\nfont_add_google(name =  \"Fuzzy Bubbles\", family = \"Fuzzy Bubbles\")\r\nshowtext_auto()\r\ntheme_clean <- function() {\r\n  theme_minimal(base_family = \"Fuzzy Bubbles\") +\r\n    theme(panel.grid.minor = element_blank(),\r\n          text = element_text(size = 20, family = \"Fuzzy Bubbles\"),\r\n          plot.background = element_rect(fill = \"white\", color = NA),\r\n          axis.text = element_text(size = 20),\r\n          axis.title = element_text(face = \"bold\", size = 24),\r\n          strip.text = element_text(face = \"bold\", size = rel(0.8), hjust = 0),\r\n          strip.background = element_rect(fill = \"grey80\", color = NA),\r\n          legend.text = element_text(size = 16))\r\n}\r\n\r\n\r\nWe’ll get our data as in the previous post.\r\n\r\n\r\ncourse_summary <- readRDS(\"../2022-08-03-golf-courses-in-donegal/data/course_summary1\") |> \r\n  mutate(course = as.character(course),\r\n         course = ifelse(course == \"Dunfanaghy\", \r\n                         glue::glue(\"<strong><p style = 'color:#56B4E9'>{course}<\/p><\/strong>\"), \r\n                         course),\r\n         course = as.factor(course))\r\ndonegal_golf <- readRDS(\"../2022-08-03-golf-courses-in-donegal/data/donegal_clean\") |> \r\n  mutate(course = as.character(course),\r\n         course = ifelse(course == \"Dunfanaghy\", \r\n                         glue::glue(\"<strong><p style = 'color:#56B4E9'>{course}<\/p><\/strong>\"), \r\n                         course),\r\n         course = as.factor(course)) |> \r\n  left_join(course_summary) |> \r\n  mutate(course = str_remove(course, \" Links\") |> as.factor())\r\nz5 <- course_summary |> \r\n  mutate(course = fct_reorder(course, total_length))\r\n\r\n\r\nNow let’s see what this tells us. First off, let’s look at the distribution of hole lengths for different pars.\r\n\r\n\r\ndonegal_golf |> \r\n  ggplot(aes(yards, col = par, group = par)) +\r\n  geom_textdensity(aes(label = glue::glue(\"Par\\n{par}\"), fill = par), stat = \"density\",\r\n                size = 12, fontface = 2, hjust = \"ymax\", vjust = 0.3, gap = T,\r\n                show.legend = F, linewidth = 2, family = \"Fuzzy Bubbles\",\r\n                lineheight = 0.5) +\r\n  geom_rug(show.legend = F) +\r\n  coord_cartesian(ylim=c(0, 0.015)) +\r\n  scale_colour_okabe_ito() +\r\n  theme_clean() +\r\n  theme(axis.text.y = element_blank(),\r\n        axis.title.y = element_blank())\r\n\r\n\r\n\r\nNote that there’s no overlap between par-3’s and par-4’s, if a hole is less than 250 yards, then it’s a par-3, otherwise it’s a par-4 or par-5. On the other hand, there is a lot of overlap between par-4’s and par-5’s. A par-4 can be long but relatively open and so feasible in 4 shots, whereas a par-5 can be shorter but tricky because of doglegs and hazards.\r\nLet’s see how long these courses are:\r\n\r\n\r\nlabel_size <- 9\r\ncourse_summary |> \r\n  mutate(course = fct_reorder(course, total_length),\r\n         length_label = ifelse(total_length == max(total_length), \r\n                               glue::glue(\"{total_length |> format(nsmall = 0)} yards: Par = {total_par}\"), \r\n                               glue::glue(\"{total_length |> format(nsmall = 0)}: {total_par}\"))) |> \r\n  ggplot(aes(course, total_length, fill = highlight)) +\r\n  geom_col(show.legend = F) +\r\n  geom_text(aes(label = length_label), \r\n            size = label_size, y = 2000,\r\n            family = \"Fuzzy Bubbles\",\r\n            fontface = \"bold\") +\r\n  scale_fill_manual(values = c(\"yes\" = \"#56B4E9\", \r\n                               \"nine\" = \"#E69F00\", \r\n                               \"eighteen\" = \"#D55E00\")) +\r\n  coord_flip() +\r\n  theme_clean() + \r\n  theme(axis.title = element_blank(),\r\n        axis.text.x = element_blank(),\r\n        axis.text.y = element_markdown(),\r\n        )\r\n\r\n\r\n\r\nA lot of variation. The nine-hole courses (in lighter orange) tend to be shorter. And, of course, the length depends on the par of the course with the par-3 course in Foyle being the shortest and Donegal being the longest and (almost) the highest par figure.\r\nLet’s try and normalise this by looking at the length of the par’s 3, 4, and 5 separately. The code is quite long, so we just show the part for the par 5’s. Note how rows have to be added for those three courses that don’t have a par 5.\r\n\r\n\r\n\r\n\r\n\r\n# par 5 average length\r\npar_5 <- donegal_golf |>\r\n  filter(par == 5) |> \r\n  group_by(course) |> \r\n  summarise(total_length = mean(yards)) |> \r\n  ungroup() |> \r\n  left_join(z5 |> select(course, highlight)) |> \r\n  mutate(length_par = total_length |> round(1),\r\n         length_label = ifelse(course == levels(z5$course) |> tail(1), \r\n                               glue::glue(\"{length_par |> format(nsmall = 1)} yards\"), \r\n                               length_par |> format(nsmall = 1)))|> \r\n  distinct() |> \r\n  add_row(course = \"Otway\", \r\n          total_length = 0, \r\n          highlight = \"nine\", \r\n          length_par = 0, \r\n          length_label = \"\") |> \r\n  add_row(course = \"Cruit Island\", \r\n          total_length = 0, \r\n          highlight = \"nine\", \r\n          length_par = 0, \r\n          length_label = \"\") |>\r\n  add_row(course = \"Foyle Centre Woodlands\", \r\n          total_length = 0, \r\n          highlight = \"nine\", \r\n          length_par = 0, \r\n          length_label = \"\") |>\r\n  mutate(course = fct_relevel(course, levels(z5$course))) |> \r\n  ggplot(aes(course, length_par, fill = highlight)) +\r\n  geom_col(show.legend = F) +\r\n  geom_text(aes(label = length_label), \r\n            size = label_size, y=200,\r\n            family = \"Fuzzy Bubbles\",\r\n            fontface = \"bold\") +\r\n  scale_fill_manual(values = c(\"yes\" = \"#56B4E9\", \r\n                               \"nine\" = \"#E69F00\", \r\n                               \"eighteen\" = \"#D55E00\")) +\r\n  coord_flip() +\r\n  theme_clean() + \r\n  theme(axis.title = element_blank(),\r\n        axis.text.x = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        panel.grid = element_blank()\r\n  )\r\n\r\nlayout <- \"AABBBCCCC\"\r\npar_3 + par_4 + par_5 + plot_layout(design = layout)\r\n\r\n\r\n\r\nIt looks like Donegal is just long. but a special mention for Portnoo, with variation that combines the shortest par 3’s with the longest par 5’s.\r\nHow does the difficulty of each hole depend on its par? To do this we use the hole index data for each course. The hole rated index 1 is considered the most difficult on a course, right down to index 18 for the easiest. In my experience, the par 3’s tend to be considered easy and so have high indices (there’s less chance of things going wrong when it’s just one shot to the green), whereas long par 4’s usually monopolise the lowest indices. Let’s see.\r\n\r\n\r\ndonegal_golf |> \r\n  group_by(index, par) |> \r\n  summarise(n = n()) |> \r\n  ungroup() |> \r\n  ggplot(aes(x = index, y = n, fill = par)) +\r\n  geom_col(show.legend = F) +\r\n  scale_x_continuous(breaks = c(3, 6, 9, 12, 15, 18)) +\r\n  scale_fill_manual(values = c(\"#009E73\", \"#D55E00\", \"#0072B2\")) +\r\n  labs(y = \"\",\r\n       title = \"Variation in Hole Index for \r\n       <strong><p style = 'color:#009E73'>Par 3's<\/p><\/strong>, \r\n       <strong><p style = 'color:#D55E00'>Par 4's<\/p><\/strong>, and \r\n       <strong><p style = 'color:#0072B2'>Par 5's<\/p><\/strong>\") +\r\n  theme_clean() +\r\n  theme(plot.title = element_markdown(size = 30),\r\n        axis.text.y = element_blank())\r\n\r\n\r\n\r\nLooks like our hunch was good, with par 3’s having an increasing presence among the higher index holes, whereas the par 4’s dominate the lower indices. Par 5’s, meanwhile, seem to have an even index probability once you get beyond index 4 or so.\r\nThis concludes our walk around the courses of Donegal. If you enjoy the game, they come highly recommended for their beauty, the varied challenges they pose, and from the warm reception afforded to visitors.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-05-golf-in-donegal-ii/images/donegal.png",
    "last_modified": "2024-06-25T18:32:36+01:00",
    "input_file": {},
    "preview_width": 381,
    "preview_height": 320
  },
  {
    "path": "posts/2022-08-03-golf-in-donegal/",
    "title": "Golf Courses in Donegal - Part I, data access",
    "description": "A quick look at the pars, lengths, and handicap indices of golf courses in Donegal and surrounds. In this first part we'll look at how to get hold of the data by some web scrapping, a second post will do some analysis of the numbers.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://www.fizzics.ie"
      }
    ],
    "date": "2022-08-03",
    "categories": [
      "Sport",
      "Donegal",
      "Web Scraping"
    ],
    "contents": "\r\nAmongst other things, Donegal is a haven for golfers. And while my favourite knock is around the home course in Dunfanaghy, I thought it worthwhile to explore the course cards for the different clubs in the county.\r\nGetting the data from each golf club would be laborious, but fortunately the website at Golfpass makes this a lot easier. As long as you can find the landing page for the golf courses you’re interested in (for Donegal it’s here) then you can strip out all the links and navigate to the courses, and their scorecards, that way. The libraries we need are tidyverse (of course) and rvest for the scraping. We end up with a lot of links, but the ones we care about are recognisable for the strings /courses/ and #write-review\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rvest)\r\n\r\nurl_donegal <- \"https://www.golfpass.com/travel-advisor/course-directory/8812-county-donegal/\"\r\n\r\nw <- read_html(url_donegal)\r\nscorecard_urls <- html_attr(html_nodes(w, \"a\"), \"href\") |> \r\n  as_tibble() |> \r\n  filter(str_detect(value, \"/courses/\"),\r\n         !str_detect(value, \"#write-review\")) |> \r\n  distinct()\r\n\r\n\r\nThe first few entries are shown below, there are 27 in total.\r\n\r\n\r\nvalue\r\n    https://www.golfpass.com/travel-advisor/courses/19794-ballybofey-and-stranorlar-golf-clubhttps://www.golfpass.com/travel-advisor/courses/19705-ballyliffin-golf-club-glashedyhttps://www.golfpass.com/travel-advisor/courses/19706-ballyliffin-golf-club-oldhttps://www.golfpass.com/travel-advisor/courses/39809-ballyliffin-golf-club-the-pollan-linkshttps://www.golfpass.com/travel-advisor/courses/25762-buncrana-golf-clubhttps://www.golfpass.com/travel-advisor/courses/19707-bundoran-golf-club\r\n\r\nNavigating through this list of url’s and stripping out the details for each hole on each course is a job for purrr. I wrote a function that works for one course at a time and then ran it through map. Twice. Then I put all the courses together using data.table::rbindlist.\r\nSome things to watch out for:\r\nsome courses had an entry in Golfpass but no scorecard. Hence the use of safely so that they didn’t crash the first call to map.\r\nthe regex set-up in the function is fundamentally geared for 18 hole courses but then needed to be adapted to cater for their 9 hole brethren. Dropping NA’s and then checking the length of each tibble in the list did this.\r\nhaving distances measured in either meters or yards was a real pain. Especially for courses that gave an overall length in yards but then each hole in meters. Or vice-versa. Summing up the lengths of the holes and then comparing to the total course length was my best work-around for this.\r\nsome of the figures from Golfpass were patently wrong. The par-5 11\\(^{th}\\) hole in Derry, for example, is obviously longer than 311 yards. I didn’t try and correct for these.\r\ncourses have different tee-boxes, championship, society, ladies, and juniors for example. I just took the championship figures, in general the longest distance tee-box for each hole.\r\nThe function that works course-by-course is shown below.\r\n\r\n\r\ncourse_card <- function(url) {\r\n  # use the url to work out the golf club name\r\n  course <- sub(\".*courses/\\\\d+-\", \"\", url) |> \r\n    str_replace_all(\"-\", \" \") |> \r\n    str_to_title()\r\n  \r\n  # get raw data from the url using rvest\r\n  w <- read_html(url)\r\n  pathway_data_html <- html_nodes(w, \"td\")\r\n  card <- html_text(pathway_data_html)\r\n  \r\n  # get distance units (m or yds) for the total length, and the total length itself\r\n  unit <- card[3] |> str_extract(\"[a-z]+\")\r\n  total_length <- card[3] |> str_extract(\"\\\\d+\") |> as.numeric()\r\n  \r\n  # work out the lengths for each hole\r\n  lengths <- card[(which(card |> str_detect(\": \\\\d\\\\d\\\\.\\\\d\"))[1]+1):(which(card |> str_detect(\": \\\\d\\\\d\\\\.\\\\d\"))[1]+20)]\r\n  # work out the handicap indices for each hole\r\n  indexs <- card[(which(card |> str_detect(\"Handicap$\"))+1):(which(card |> str_detect(\"Handicap$\"))+20)]\r\n  # work out the pars (3, 4, or 5) for each hole\r\n  par <- card[(which(card |> str_detect(\"Par\"))+1):(which(card |> str_detect(\"Par\"))+20)]\r\n  \r\n  # put all this together into a tibble\r\n  my_card <- tibble(course = rep(course, 18),\r\n                        hole = 1:18,\r\n                        par = c(par[1:9], par[11:19]) |> as.numeric(),\r\n                        length = c(lengths[1:9], lengths[11:19]) |> as.numeric(),\r\n                        index = c(indexs[1:9], indexs[11:19]) |> as.numeric()\r\n                    )\r\n  # correct for 9 hole courses\r\n  my_card <- my_card |> drop_na()\r\n  if(dim(my_card)[1] < 18) {\r\n    my_card = filter(my_card, hole<=9)\r\n  } \r\n  hole_lengths <- sum(my_card$length)\r\n  if(dim(my_card)[1] == 9) {\r\n    hole_lengths = 2 * hole_lengths\r\n  }\r\n  \r\n  # total length given in yards but holes in meters\r\n  if(total_length > hole_lengths * 1.05){\r\n    my_card <- my_card |> \r\n      mutate(unit = \"meters\",\r\n             yards = length * 1.09361)\r\n  }\r\n  \r\n  # total length given in meters but holes in yards\r\n  if(total_length < hole_lengths * 1.05){\r\n    my_card <- my_card |> \r\n      mutate(unit = \"yards\",\r\n             yards = length)\r\n  }\r\n\r\n  my_card |> select(course, hole, par, length, unit, index, yards)\r\n}\r\n\r\n\r\n\r\n\r\n\r\nAnd the calls to map to create the overall data frame are shown here.\r\n\r\n\r\nz1 <- map(scorecard_urls$value, safely(course_card))\r\nz2 <- map(1:length(z1), function(x) z1[[x]]$result)\r\ndonegal_golf <- data.table::rbindlist(z2) |> \r\n  mutate(par = as.factor(par))\r\n\r\n\r\nFinally, we’ll do a little cleaning. We know these are all golf courses, so we can skip that part of the name. Also, for nine hole courses, players tend to go around twice, so we’ll double up their holes and adjust the hole indices accordingly.\r\n\r\n\r\ndonegal_golf <- donegal_golf |> \r\n    mutate(course = course |> str_remove(\"Golf\") |> \r\n           str_remove(\"Links\") |>\r\n           str_remove(\"Club\") |> \r\n           str_remove(\"And\") |> \r\n           str_remove(\"Hotel\") |> \r\n           str_remove(\"International\") |> \r\n           str_squish())\r\n\r\ncourse_summary <- donegal_golf |>\r\n  group_by(course) |> \r\n  summarise(total_length = sum(yards),\r\n            total_par = sum(par |> as.character() |> as.numeric()),\r\n            highlight = ifelse(n() < 10, \"nine\", \"eighteen\")) |> \r\n  ungroup() \r\ncourse_summary <- course_summary |> \r\n  mutate(highlight = ifelse(course == \"Dunfanaghy\", \"yes\", highlight))\r\n\r\nnines <- course_summary |> \r\n  filter(highlight == \"nine\")\r\n\r\nnine_cards <- donegal_golf |> \r\n  filter(course %in% nines$course)\r\n\r\nnine_type <- nine_cards |> \r\n  group_by(course) |> \r\n  summarise(index_type = sum(index)) |> \r\n  ungroup()\r\n\r\nnine_cards <- nine_cards |> \r\n  left_join(nine_type) |> \r\n  mutate(index = case_when(index_type == 81 ~ index+1,\r\n                           index_type == 90 ~ index-1,\r\n                           index_type == 45 ~ index*2),\r\n         hole = hole + 9) |> \r\n  select(-index_type)\r\n\r\notway_style_indices <- nine_type |> filter(index_type == 45) |> pull(course)\r\n\r\ndonegal_golf <- donegal_golf |> \r\n  mutate(index = ifelse(course %in% otway_style_indices, index*2 - 1, index)) |> \r\n  bind_rows(nine_cards)\r\n\r\n\r\nThis gives a tibble that looks like this:\r\n\r\n\r\n\r\nNow we have our data the way we want, we can look and see what this tells us. That’s for the next post\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-03-golf-in-donegal/images/dfgc.jpg",
    "last_modified": "2024-06-25T15:52:22+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-22-patchework-and-gt/",
    "title": "patchwork & gt tables",
    "description": "gt makes beautiful tables, and patchwork is a wonderful package for combining plots. This post shows a way to make them work together.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://www.fizzics.ie"
      }
    ],
    "date": "2022-02-22",
    "categories": [
      "Academia",
      "R"
    ],
    "contents": "\r\n\r\n\r\n\r\nThere are many options to make tables in R (knitr::kable, formattable, flextable….), but gt is pretty much my favourite. It makes magnificent tables, and closely follows the philosophy, if not the syntax, of ggplot.\r\nOne problem with gt tables, is that they don’t sync well with patchwork. This is because patchwork needs grobs, and gt doesn’t do that. For most types of tables, the gridExtra::tableGrob function would do the job, but not so for gt tables. This post presents a workaround (well, really a couple of fudges), so that we can get around this. We can use the beauty of gt tables, with all the flexibility of patchwork layouts.\r\nFirst off, I’d like all our tables to have the same look. So I’m going to create a common theme to apply to each of them. This is our first little hack here, as I haven’t seen this done elsewhere.\r\n\r\n\r\nmy_theme <- function(data){\r\n  data %>% \r\n    opt_table_font(font = google_font(\"Annie Use Your Telescope\")) %>% \r\n    tab_options(column_labels.background.color = \"#176940\",\r\n                table.font.size = px(16)\r\n    )\r\n}\r\n\r\n\r\n\r\nNext, let’s generate some tables. Let’s use the familiar penguins dataset from the palmerpengiuns package, and make the following tables:\r\n\r\n\r\ntable1 <- penguins %>% \r\n  group_by(species) %>% \r\n  summarise(weight = mean(body_mass_g, na.rm = T) %>% signif(3)/1000) %>% \r\n  ungroup() %>% \r\n  gt() %>% \r\n  my_theme()\r\ntable1\r\n\r\n\r\n\r\nspecies\r\n      weight\r\n    Adelie\r\n3.70Chinstrap\r\n3.73Gentoo\r\n5.08\r\n\r\ntable2 <- penguins %>% \r\n  group_by(species) %>% \r\n  summarise(bill_length = mean(bill_length_mm, na.rm = T) %>% signif(3)) %>% \r\n  ungroup() %>%\r\n  gt() %>% \r\n  my_theme()\r\ntable2\r\n\r\n\r\n\r\nspecies\r\n      bill_length\r\n    Adelie\r\n38.8Chinstrap\r\n48.8Gentoo\r\n47.5\r\n\r\ntable3 <- penguins %>% \r\n  group_by(year, island) %>% \r\n  summarise(weight = mean(body_mass_g, na.rm = T) %>% signif(3)/1000) %>% \r\n  ungroup() %>%\r\n  gt() %>% \r\n  my_theme()\r\ntable3  \r\n\r\n\r\n\r\nyear\r\n      island\r\n      weight\r\n    2007\r\nBiscoe\r\n4.742007\r\nDream\r\n3.682007\r\nTorgersen\r\n3.762008\r\nBiscoe\r\n4.632008\r\nDream\r\n3.782008\r\nTorgersen\r\n3.862009\r\nBiscoe\r\n4.792009\r\nDream\r\n3.692009\r\nTorgersen\r\n3.49\r\n\r\nNow for our second hack. We save each table in turn as a tmp file and then read it back in as a .png. This idea is from Johannes Enevoldsen given at https://github.com/rstudio/gt/issues/420.\r\nAnd now the final hack. It seems like patchwork needs to kick off with an actual ggplot object, we can’t just combine tables. So we create a blank plot and then make sure it’s underneath everything when we apply the patchwork::plot_layout.\r\n\r\n\r\nlayout <- c(\r\n  area(t=1, l=2, b=2, r=2),\r\n  area(t=0, l=0, b=3, r=2),\r\n  area(t=4, l=0, b=6, r=2),\r\n  area(t=0, l=3, b=6, r=6)\r\n)\r\n\r\n(ggplot() + theme_void()) + table_png1 + table_png2 + table_png3 + \r\n  plot_layout(design = layout)\r\n\r\n\r\n\r\n\r\nAnd there you have it. Of course, this is just a rudimentary layout, we haven’t even begun to harness the full repetoire of either gt or patchwork. But it’s a start.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-22-patchework-and-gt/patchework-and-gt-tables_files/figure-html5/patches-1.png",
    "last_modified": "2023-01-17T14:13:35+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-14-primel/",
    "title": "Primel",
    "description": "Primel is the numeric equivalent of wordle, using 5 digit prime numbers instead of words. This post un-picks this in R.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://www.fizzics.ie"
      }
    ],
    "date": "2022-02-16",
    "categories": [
      "R",
      "Fun"
    ],
    "contents": "\r\nIn our house, we’re fans of wordle, sitting down every evening to keep our streak going. But there is also a number equivalent called primel, using five digit prime numbers instead of five letter words. This, of course, is therefore prefect territory for some R programming. Let’s look at yesterday’s primel challenge in this post.\r\nLike wordle, getting the appropriate initial guess is key. It has to be a prime number, and like in wordle, it’s good not to repeat digits. The available space of numbers range from 10000 to 99999. Let’s make a vector of these and then filter to just have the prime numbers. The isprime() function from the matlab library is useful here.\r\n\r\n\r\nnumbers <- tibble(x = 10000:99999) %>% \r\n  filter(matlab::isprime(x) == TRUE)\r\n\r\n\r\n\r\nThis leaves us with 8363 prime numbers.\r\nNow we want to get rid of numbers that repeat a digit. 11113 wouldn’t be a great initial guess as it contains four 1’s. To do this, we’ll split the numbers down to their individual digits, make a table of each one, and discard numbers which have a table entry greater than 1.\r\n\r\n\r\nnumbers <- numbers %>% \r\n  mutate(x_split = str_split(x, \"\"))\r\nindices_no_repeats <- map(numbers$x_split, function(x){table(x) %>% max() == 1}) %>% unlist()\r\nnumbers <- numbers[indices_no_repeats,]\r\nhead(numbers$x)\r\n\r\n\r\n[1] 10243 10247 10253 10259 10267 10273\r\n\r\nWe end up with 2529 prime numbers with unique digits. The first one of these is 10243. So let’s plug that in to today’s primel.\r\n\r\n\r\n\r\n1, 0, and 3 came up as correct digits, but in each case in the wrong place. So let’s try primes that start with 3, and finish with 01. To do this, we set up a vector of numbers from 30001 to 40000, with a step size of 100. Then we check which ones are primes. We end up with 17 possibilities. After our initial guess, we can exclude ones that include the digits 2 and 4, and again we’d prefer ones with no repeat digits. 35801 is the first in our list to satisfy these criteria. Let’s try it.\r\n\r\n\r\nbegin <- 30001\r\nend <- 40000\r\nmy_step <- 100\r\n\r\nnumbers <- seq(begin, end, by = my_step)\r\n\r\nnumbers[isprime(numbers)==1]\r\n\r\n [1] 31601 32401 32801 33301 33601 34301 34501 35201 35401 35801 36901\r\n[12] 37201 37501 38201 38501 39301 39901\r\n\r\n\r\n\r\n\r\nFrom these results, we see that 3 and 1 are indeed the first and last digits, and 0 must be the middle digit. Let’s run this again to see which primes satisfy these conditions. Note, the numbers%%1000 picks out the last three digits of our numbers, and checking that this is between 0 and 100 means the third last digit must be a 0.\r\n\r\n\r\nbegin <- 30001\r\nend <- 40000\r\nmy_step <- 10\r\n\r\nnumbers <- seq(begin, end, by = my_step)\r\n\r\nnumbers[isprime(numbers)==1 & between(numbers%%1000, 0, 100)]\r\n\r\n [1] 30011 30071 30091 31051 31081 31091 32051 33071 33091 34031 34061\r\n[12] 35051 35081 36011 36061 37021 37061 38011 39041\r\n\r\nWe can exclude numbers with the digits 2, 4, 5, or 8, and again, we’d like to have a guess with no repeat digits. 37061 looks good, let’s try it.\r\n\r\n\r\n\r\nNow we know that the first three digits must be 360, and the last digit must be 1. Let’s generate those primes.\r\n\r\n\r\nbegin <- 36001\r\nend <- 36100\r\nmy_step <- 10\r\n\r\nnumbers <- seq(begin, end, by = my_step)\r\n\r\nnumbers[isprime(numbers)==1]\r\n\r\n[1] 36011 36061\r\n\r\nThis gives us just two possibilities, but we’ve already ruled out 6 in the fourth position from guess three, so our answer must be 36011. And sure enough.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-14-primel/images/first-guess.png",
    "last_modified": "2023-09-13T14:38:13+01:00",
    "input_file": {},
    "preview_width": 527,
    "preview_height": 524
  },
  {
    "path": "posts/2021-12-27-jupiter-trojans/",
    "title": "Jupiter Trojans",
    "description": "NASA's Lucy mission launched earlier this year, setting out to examine the Jupiter Trojans. We'll have a look at them now.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-12-27",
    "categories": [
      "Physics",
      "R"
    ],
    "contents": "\r\nThe Jupiter Trojans are two groups of asteroids, beyond the main asteroid belt in stable orbits at the L4 and L5 Lagrange points of Jupiter. This means that they are about the same distance from the Sun as Jupiter (5.2AU), and either \\(60^{\\circ}\\) ahead (L4) or behind (L5) Jupiter’s orbit.\r\nThere are lots of them, comparable in number to the main asteroids. The larger ones are named and follow a convention using Greek (L4) or Trojan (L5) warriors. Except for the early ones, leading to some spies in the opposite camps.\r\nUntil Lucy reports back over the next few years, all we know about them comes from observations from Earth. We get a fascinating glimpse of their story. They look old and unprocessed, dark carbonaceous chondrites. They lend weight to the Nice Model, making the Trojans kindred spirits to Kuiper Belt objects. You can see how results from Lucy are anticipated with a lot of excitement.\r\nSo let’s have a look. Our data come from two sources; the JPL Small Bodies Database, and the Minor Planet Centre. The data from the Minor Planet Centre we web scraped, but for JPL it was easier just to download a csv with data, limiting it to the Trojans and choosing pretty much all the data fields available. Joining the tables together was tricky, and required using a mixture of both designation and provisional designation.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nz <- read_csv(\"data/nasa-small-bodies-database.csv\") %>%\r\n  mutate(first_obs = as.Date(first_obs),\r\n         designation = str_extract(full_name, pattern = \"(?<=\\\\().*(?=\\\\))\")) %>%\r\n  dplyr::select(designation, pdes, name, spkid, diameter, albedo,\r\n                rot_per, ma, n, first_obs)\r\n\r\nurl <- \"https://www.minorplanetcenter.net/iau/lists/JupiterTrojans.html\"\r\n\r\nxml <- \"pre\"\r\n\r\nw <- rvest::read_html(url)\r\npathway_data_html <- rvest::html_nodes(w, xml)\r\nev <- rvest::html_text(pathway_data_html) %>%\r\n  str_split(\"\\n\") %>%\r\n  unlist()\r\ntjn <- ev[-c(1, 3, 10794)] %>%\r\n  str_sub(start = 1, end = 8)\r\ntjn <- tjn[-1] %>% \r\n  str_remove(\"\\\\(\") %>% \r\n  str_remove(\"\\\\)\") %>% \r\n  str_remove(\" +\")\r\nev <- ev[-c(1, 3)] %>%\r\n  str_sub(start = 28) %>%\r\n  str_replace_all(\" +\", \" \")\r\n\r\ntrojans <- read_table(ev) %>%\r\n  janitor::clean_names() %>%\r\n  unite(col = design, prov:des, sep = \" \", remove = F) %>% \r\n  mutate(pdes = tjn)\r\n\r\nz <- z %>% left_join(trojans %>% select(pdes, design))\r\n\r\ntrojans <- trojans %>%\r\n  select(-pdes) %>% \r\n  left_join(z) %>% \r\n  filter(h < 30)\r\n\r\n\r\n\r\n\r\n\r\n\r\nTrojan Orbits\r\nLet’s first off look at where the Trojans are. Below we show the distributions for aphelion and perihelion distance, and the relationship between them.\r\n\r\n\r\n\r\nNothing here seems too startling, and the eccentricities for L4 and L5 seem similar.\r\nIn the main asteroid belt we see evidence of several collisional families, asteroids groups with a narrow range of inclinations along a strip of semi-major axes. These are all formed by some impact in the past. Let’s’ see if there is evidence of this in the Trojans.\r\n\r\n\r\n\r\nWe see no sign of these collisional families here. This squares with the Nice model, the origin of the Trojans is late capture of Kuiper Belt objects. The timing means that they avoided most of the maelstrom of early Solar System formation.\r\nLooking at the inclinations of these asteroids, we can see that L5 Trojans tends to have higher inclinations. This is something that has been noticed already but not yet explained.\r\n\r\n\r\n\r\nTrojan Physical Properties\r\nThe Trojans are dark, much darker than the stony (S type) or metallic (M type) asteroids that we frequently find in the main asteroid belt. Let’s see how this stacks up with our data, examining the albedo of our Trojans.\r\n\r\n\r\n\r\nWe’re seeing albedos typically around 0.05-0.1, with some very dark bodies indeed, and a minimum albedo of 0.011.\r\n\r\n\r\n\r\nWe see typical periods of 1o hours, the Solar Systems favourite period, but some slow rotators with periods up to 1000 hours.\r\nFinally, let’s look at the Trojan asteroid size distribution. Lots of these asteroids don’t have diameters in the minor planet centre database, so we’ve imputed then based on absolute magnitude and this look up table. The table gives sizes in three units (kilometres, metres, and millimetres), for \\(\\frac12\\) integer absolute magnitude values. We did a loess interpolation to fit the actual absolute values to this curve.\r\n\r\n\r\n# sizing\r\nurl_size <- \"https://www.minorplanetcenter.net/iau/Sizes.html\"\r\nw <- rvest::read_html(url_size)\r\npathway_data_html <- rvest::html_nodes(w, xml)\r\nev <- rvest::html_text(pathway_data_html) %>%\r\n  str_split(\"\\n\") %>%\r\n  unlist()\r\nev <- ev[-c(1:4, 6, 37:47)] %>% \r\n  str_replace_all(\" - \", \"   \") %>% \r\n  str_replace_all(\" +\", \" \")\r\nev[1] <- \"H1 D_min_icy D_min D_max H2 H3\"\r\ntrojan_dist <- read_table(ev) %>% \r\n  pivot_longer(cols = c(H1, H2, H3)) %>% \r\n  mutate(new_diameter = case_when(name == \"H1\" ~ (D_min + D_max)/2,\r\n                           name == \"H2\" ~ (D_min + D_max)/2/1e3,\r\n                           name == \"H3\" ~ (D_min + D_max)/2/1e6)) %>% \r\n  arrange(name, value) %>% \r\n  select(h = value, everything()) %>% \r\n  select(-name)\r\n\r\ncalibration <- loess(new_diameter ~ h, data = trojan_dist, span = 0.25)\r\ntrojans <- trojans %>% \r\n  mutate(new_diam = predict(calibration, newdata = trojans), \r\n         diameter = ifelse(is.na(diameter), new_diam, diameter))\r\n\r\n\r\n\r\nThen we can produce the plot as shown below:\r\n\r\n\r\n\r\nAnd the data\r\nThis is an excerpt from the data table, all the asteroids but not all the columns\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-27-jupiter-trojans/jupiter-trojans_files/figure-html5/distances-1.png",
    "last_modified": "2023-01-17T14:13:35+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1536
  },
  {
    "path": "posts/2021-12-16-sdss-data-access/",
    "title": "SDSS Data Access",
    "description": "How to get hold of data from the SDSS astronomical catalogue.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://www.fizzics.ie"
      }
    ],
    "date": "2021-12-16",
    "categories": [
      "Physics",
      "R"
    ],
    "contents": "\r\nWhen it comes to astronomical catalogues, SDSS was without peer for many years. We used it routinely for our work on differential photometry, and developed ways of accessing SDSS data directly to R. This post is really just a reminder-to-self, so that next time I need to get my SDSS data I won’t need to scrabble around looking for old scripts.\r\nThe packages needed are tidyverse (of course), RCurl, and glue.\r\n\r\n\r\n\r\n\r\n\r\nN <- 5 # makes a table of 5 star targets\r\n# we'll just get the spectrum from the first one\r\n# delta <- 0.1 # \r\nbands_min <- 15 # this places an upper limit on the brightness of the star\r\nbands_max <- 20 # this places an lower limit on the brightness of the star\r\n# fainter stars have noisy spectra\r\n\r\n\r\n\r\n\r\n# SQL that downloads some info on the chosen target from SDSS.\r\n# ObjID from SDSS specifies the target\r\n\r\nmaster_target_SqlQuery <- glue(\"SELECT top {N} p.ra, p.dec, \",\r\n                       \"p.u, p.g, p.r, p.i, p.z, p.objid, \", \r\n                       \"s.specobjid, s.class, s.subclass, s.survey, \", \r\n                       \"s.plate, s.mjd, s.fiberid \", \r\n                       \"FROM photoObj AS p \", \r\n                       \"JOIN SpecObj AS s ON s.bestobjid = p.objid \",\r\n                       \"WHERE p.g BETWEEN {bands_min} AND {bands_max} \",\r\n                       \"AND p.r BETWEEN {bands_min} AND {bands_max} \", \r\n                       \"AND p.i BETWEEN {bands_min} AND {bands_max} \", \r\n                       \"AND s.class = 'STAR' \",\r\n                       \"AND s.survey != 'eboss'\" )\r\n\r\n\r\n\r\n\r\n# downloads target data\r\n# dataframe master_targets has necessary info\r\nmaster_target_SqlQuery <- str_squish(master_target_SqlQuery) |> \r\n  str_replace_all(\" \", \"%20\")\r\nmy_format <- \"&format=csv\"\r\nurlBase <- \"http://skyserver.sdss.org/dr16/SkyServerWS/SearchTools/SqlSearch?cmd=\"\r\nmaster_targets <- read_csv(paste0(urlBase, master_target_SqlQuery, my_format), skip = 1)\r\n\r\n\r\n\r\n\r\n\r\nra\r\n      dec\r\n      u\r\n      g\r\n      r\r\n      i\r\n      z\r\n      objid\r\n      specobjid\r\n      subclass\r\n      plate\r\n      mjd\r\n      fiberid\r\n    245.1802\r\n6.98543\r\n16.73\r\n15.56\r\n15.78\r\n15.96\r\n16.06\r\n1237662636377833472\r\n1950104876876851200\r\nA0\r\n1732\r\n53501\r\n168245.2807\r\n6.83412\r\n21.76\r\n19.48\r\n18.13\r\n17.02\r\n16.43\r\n1237662199901913344\r\n1950105151754758144\r\nM3\r\n1732\r\n53501\r\n169245.2640\r\n7.33773\r\n19.19\r\n17.89\r\n17.39\r\n17.22\r\n17.15\r\n1237662200438718720\r\n1950105426632665088\r\nF9\r\n1732\r\n53501\r\n170245.2317\r\n7.38190\r\n21.69\r\n19.30\r\n17.88\r\n16.95\r\n16.42\r\n1237662636914639616\r\n1950106251266385920\r\nM2\r\n1732\r\n53501\r\n173245.2420\r\n7.31339\r\n21.93\r\n19.31\r\n17.92\r\n16.92\r\n16.39\r\n1237662200438718720\r\n1950107075900106752\r\nM2\r\n1732\r\n53501\r\n176\r\n\r\nThere are times when the SDSS data server is down. In this case, expect to see an error message like - “Error: InternalServerError”.\r\nNow that we have the plate, mjd, and fiberid for some stars, we can go ahead and download their spectra.\r\n\r\n\r\nindex <- 1 # uses first star from list\r\nget_spectrum <- function(object, wavelength_lower_limit = 5500, wavelength_upper_limit = 7000){\r\n  plate <- object$plate\r\n  mjd <- object$mjd\r\n  fiber <- object$fiberid\r\n  url_spect <- glue(\"http://dr12.sdss.org/csvSpectrum?plateid={plate}\", \r\n                    \"&mjd={mjd}&fiber={fiber}&reduction2d=v5_7_0\")\r\n  spectrum <- read_csv(file = url_spect)\r\n  spectrum %>% \r\n    filter(between(Wavelength, wavelength_lower_limit, wavelength_upper_limit)) %>% \r\n    dplyr::select(Wavelength, BestFit)\r\n}\r\nspect1 <- get_spectrum(master_targets[index,], \r\n                       wavelength_lower_limit = 3500, \r\n                       wavelength_upper_limit = 8000)\r\n\r\n\r\nThis gives a dataframe with wavelength and intensity. Let’s plot this\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe spectrum from sdss is shown below, and can be seen here\r\nSDSS Spectrum RA = 245.1801662, Dec = 6.9854337The SDSS image of the star itself is given here\r\n\r\nFull code is available from github.\r\nNote, for some reason the SDSS objid returned from the SQL query is incorrect. I’m still trying to figure out why and to fix this.\r\nUpdate, the following is an ugly workaround on the objid issue. Haven’t verified that it always works yet.\r\n\r\n\r\nradial_url_root <- \"http://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch?cmd=\"\r\nradial_url_core <- glue(\"SELECT top {N} p.ra, p.dec, \",\r\n                        \"p.u, p.g, p.r, p.i, p.z, p.objid, \", \r\n                        \"s.specobjid, s.class, s.subclass, s.survey, \", \r\n                        \"s.plate, s.mjd, s.fiberid \", \r\n                        \"FROM photoObj AS p \", \r\n                        \"JOIN SpecObj AS s ON s.bestobjid = p.objid \",\r\n                        \"WHERE p.g BETWEEN {bands_min} AND {bands_max} \",\r\n                        \"AND p.r BETWEEN {bands_min} AND {bands_max} \", \r\n                        \"AND p.i BETWEEN {bands_min} AND {bands_max} \", \r\n                        \"AND s.class = 'STAR' \",\r\n                        \"AND s.survey != 'eboss'\" ) %>% \r\n  str_replace_all(\" \", \"%20\") %>% \r\n  str_replace_all(\"\\n\", \"\")\r\nw <- rvest::read_html(glue::glue(radial_url_root, radial_url_core, \"&format=csv\"))\r\nX <- as_list(w)$html$body$p[[1]] %>% \r\n  as.character() %>% \r\n  str_remove(\"#Table1\\n\")\r\nmaster_targets <- read.table(text = X, header = TRUE, sep = \",\", dec = \".\", comment.char = \"#\") %>% \r\n  mutate(across(where(is.numeric), round, 2),\r\n         objid = as.character(objid),\r\n         specobjid = as.character(specobjid))\r\nmaster_targets$objid # check the top one to see if this works\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-16-sdss-data-access/sdss-data-access_files/figure-html5/plot_spectrum-1.png",
    "last_modified": "2024-07-17T21:36:42+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-12-15-university-points/",
    "title": "University Points",
    "description": "How do entry level points vary by university. And a chance to play with geomtextpath.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netliffy.app"
      }
    ],
    "date": "2021-12-15",
    "categories": [
      "Academia",
      "R"
    ],
    "contents": "\r\nWe looked at university choices last week, and the way in which different third level courses have different entry requirements. Using the same data, I thought it might be fun to see which were the most sought after universities. It’s also an excuse to play with geomtextpath, a package I haven’t used before but that looks like it has some nifty features for ggplot2 work.\r\nWe get the data as in the university choices post.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis gives a data frame something like this:\r\n\r\n\r\n\r\n\r\n\r\nz %>% filter(college %in% c(\"TR\", \"TU\", \"MH\")) %>% \r\n  ggplot(aes(x = final, colour = name)) +\r\n  geom_textpath(aes(label = name), stat = \"density\",\r\n                size = 12, fontface = 2, hjust = 0.2, vjust = 0.3,\r\n                show.legend = F, linewidth = 2, family = \"Fuzzy Bubbles\") +\r\n  scale_color_okabe_ito() +\r\n  theme_clean() +\r\n  theme(axis.title.y = element_blank(),\r\n        axis.text.y = element_blank()) +\r\n  coord_cartesian(xlim = c(200, 700))\r\n\r\n\r\n\r\n\r\nLooks like a clear pecking order, with Trinity College Dublin top of the pile, colleges like Maynooth University in the middle, and the newer colleges like Technological University Dublin catching up.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-15-university-points/university-points_files/figure-html5/figure_out-1.png",
    "last_modified": "2023-01-17T14:13:35+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1152
  },
  {
    "path": "posts/2021-12-06-university-choices/",
    "title": "University Choices",
    "description": "Navigating through the college choice options from the CAO.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netliffy.app"
      }
    ],
    "date": "2021-12-06",
    "categories": [
      "Academia",
      "R"
    ],
    "contents": "\r\nI have a daughter finishing up school and so looking at her university options for next year. In Ireland, there is a central clearing house (called the CAO) that takes care of university admission based on performance in an end-of-school exam. This exam gives you a certain number of points, with a maximum of 625.\r\nThe preference (at the moment, it changes frequently) is for something featuring Environmental Science. To that end, we looked around at the different options available at Irish universities and colleges. There are about 10 Irish universities, an amazing number for a small country, but then we have a strong academic ethos here. There are also a similar number of smaller colleges. Looking through the various offerings, there are North of 1000 different courses available for students wishing to undertake a four year degree. Quite the range of choices.\r\nNext question is, which ones feature Environmental Science? To find out, we went to the CAO website. There you can track down and access a bunch of pdf’s and excel spreadsheets that detail the various courses, as well as the points required for entry. And it gives this information going back several years. A data treasure trove.\r\nOf course, pdf’s and excel spreadsheet are not usually designed for easy access for data mining. The pdftools package helps a lot, but there is still plenty of data wrangling to do to get things in to middling half decent shape\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nyears <- c(\"07\", \"08\", \"09\", \"10\", \"13\", 14:19 )\r\ncao_points_year <- function(year) {\r\n  cao_pdf <- glue::glue(\"http://www2.cao.ie/points/lvl8_{year}.pdf\")\r\n  z <- pdf_text(cao_pdf) %>% \r\n    str_split(\"\\n\") %>% \r\n    unlist()\r\n  z <- z[!str_detect(z, \"^ \") & z != \"\" & !str_detect(z, \"Course Code\") & str_count(z, \"  +\") == 3] %>% # gets rid of non-data rows\r\n    str_split(\"  +\") %>% # splits rows based on runs of several spaces\r\n    unlist() %>% \r\n    str_remove(\"#\") %>% \r\n    str_remove(\"\\\\*\") # deletes some annoying characters\r\n  z <- tibble(year = glue::glue(\"20{year}\"), \r\n              code = z[c(T, F, F, F)], \r\n              course = z[c(F, T, F, F)], \r\n              final = z[c(F, F, T, F)], \r\n              medium = z[c(F, F, F, T)]) \r\n  # original list made bunches of four elements that together described a course. This tibble winds them up to one row\r\n  z\r\n}\r\n\r\nz <- map_df(years, cao_points_year) %>% \r\n  mutate(year = as.numeric(year))\r\n\r\n\r\n\r\nThat took care of the pdf’s. But the last two years (2020, 2021) data was stored in excel files. It turned out to be simpler to download these from here and here and then whip them into the same shape as our pdf data.\r\n\r\n\r\nz21 <- readxl::read_excel(\"data/CAOPointsCharts2021.xlsx\", \r\n                        sheet = \"EOS_2021\", skip = 10) %>% \r\n  janitor::clean_names() %>% \r\n  filter(course_level == 8) %>% \r\n  mutate(year = 2021) %>% \r\n  select(year,\r\n         code = course_code, \r\n         course = course_title,\r\n         final = eos_points,\r\n         medium = eos_midpoints) %>% \r\n  mutate(year = 2021,\r\n         medium = as.character(medium))\r\nz20 <- readxl::read_excel(\"data/CAOPointsCharts2020.xlsx\", \r\n                        sheet = \"PointsCharts2020_V2\", skip = 9) %>% \r\n  janitor::clean_names() %>% \r\n  filter(level == 8) %>% \r\n  mutate(year = 2020) %>%  \r\n  select(year,\r\n         code = course_code2, \r\n         course = course_title,\r\n         final = eos,\r\n         medium = eos_mid_point) %>% \r\n  mutate(year = 2020)\r\n\r\n\r\n\r\nAnd finally, we put all three together\r\n\r\n\r\nz <- bind_rows(z, z20, z21)\r\n\r\n\r\n\r\n\r\n\r\n\r\nNow we take this data (all 10585 rows of it), pick out the courses with an Environmental Science -like hue, and create a plot showing how the entry points have changed over the years. The results is shown below. The labels on the graph give the course code, which also points toward the host university. So TR064 will indicate a course in Trinity College Dublin, GY308 a course in Galway University. The actual names of the courses can be pretty verbose and have been abbreviated in the legend. It’s still possible, just about, to guess what they are, Blgcl,ErthandEnvSc,s for example, would be Biological, Earth and Environmental Sciences.\r\n\r\n\r\n\r\nI’m not sure if we’re any closer to pinpointing a course, but at least with this analysis it’ll be a lot easier to investigate next weeks favourites.\r\nThe code for the plot is given below:\r\n\r\n\r\nz %>%  filter(code %in% courses) %>%  \r\n  left_join(max_years) %>% \r\n  mutate(last_name = str_replace(last_name, \"Environmental Science\", \"EnvSci.\")) %>% \r\n  mutate(label = ifelse(year == year_max, code, \"\")) %>% \r\n  mutate(code = glue::glue(\"{code}: {abbreviate(last_name, 20)}\")) %>% \r\n  ggplot(aes(year, final, colour = code, group = code)) +\r\n  geom_line(size = 2) +\r\n  geom_point(size = 5) +\r\n  geom_label_repel(aes(label = label),\r\n                   nudge_x = 0.2,\r\n                   size = 12,\r\n                   na.rm = TRUE,\r\n                   show.legend = F) +\r\n  scale_color_okabe_ito() +\r\n  scale_y_continuous(breaks = seq(100, 700, by = 50)) +\r\n  labs(y = \"Final Points\", x = \"\") +\r\n  theme_clean() +\r\n  theme(legend.position = \"bottom\", legend.title = element_blank()) +\r\n  guides(col=guide_legend(nrow=3))\r\n\r\n\r\n\r\nAnd the libraries used are:\r\n\r\n\r\n# http://www2.cao.ie/points/deg03.htm#trd\r\n\r\nlibrary(tidyverse)\r\nlibrary(pdftools)\r\nlibrary(ggokabeito)   # Colorblind-friendly color palette\r\nlibrary(showtext)\r\nlibrary(ggrepel)\r\n\r\n\r\n\r\nFull code can be seen on github.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-06-university-choices/university-choices_files/figure-html5/plot-1.png",
    "last_modified": "2023-01-17T14:13:34+00:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 1344
  },
  {
    "path": "posts/2021-10-27-halloween-poster/",
    "title": "Halloween Poster",
    "description": "Giving a COVID poster that halloween vibe.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-10-27",
    "categories": [
      "Images",
      "Fun"
    ],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nCovid-19 is a lethal pandemic, and our thoughts go out to those who have lost loved-ones or livelihoods to the disease.\r\nThat said, this is a special time of year, so we felt appropriate to give coronavirus a halloweenesque (halloweenable, halloweenish?) treatment. We’re going to make a Halloween poster featuring the virus, and we’re going to do it with magick. Of course.\r\nMaterials and Methods\r\nFonts: the run-of-the-mill fonts just don’t quite do it for halloween. So we turned to fonts4free and their line-up of sinister fonts. Ghastly Panic and Gypsy Curse, and Fiendish looked about right, with an honourable mention to October Crow. The extrafont package is pretty good at handling the installation of new fonts, but for these three I just downloaded the .ttf files and put them in my fonts directory.Background: There are lots corona posters out there. We chose one from the Health Service Executive here in Ireland. We downloaded it and put it in a local directory.\r\nThe Code\r\nFirst we put in place the libraries we’ll need:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(magick)\r\nlibrary(showtext)\r\nlibrary(emojifont)\r\nlibrary(here)\r\n\r\n\r\n\r\ntidyverse is an ever present, magick does the heavy lifting, showtext to access the new fonts, emojifont to get some Halloween icons, and here because it is so useful.\r\nNext we’ll load up those fonts.\r\n\r\n\r\nfont_add(\"Ghastly Panic\", \"fonts/Font1/Ghastly Panic.ttf\")\r\nfont_add(\"Gypsy Curse\", \"fonts/Font2/Gypsy Curse.ttf\")\r\nfont_add(\"Fiendish\", \"fonts/Font3/Fiendish.ttf\")\r\n\r\n\r\n\r\nNow we can start to build our poster. We use magick to read in the hse image file. It initially has a white border that is then cropped out. On top on this we place a yellow block and a white block to cover existing Coronavirus and Covid-19 text (we had discerned the exact shade of yellow beforehand, thank you colorfindr), and a white block to cover other text further down the poster. Then we replace these with the same text in our Halloween fonts.\r\n\r\n\r\ncorona <- image_read(\"images/covid.jpg\") %>% \r\n  image_crop(\"625x900+185+50\") %>% \r\n  image_draw()\r\n# get rid of existing text by putting colour blocks over them\r\nrect(xleft = 0, ybottom = 150, xright = 400, ytop = 0, border = NA, col = \"#FFEE00\")\r\nrect(xleft = 80, ybottom = 850, xright = 545, ytop = 680, border = NA, col = \"#FFFFFF\")\r\n# add new text in appropriate fonts\r\ntext(x = 200, y = 100, family = \"Gypsy Curse\", cex = 8,\r\n     label = \"Coronavirus\")\r\ntext(x = 130, y = 200, family = \"Ghastly Panic\", cex = 6, \r\n     label = \"Covid-19\")\r\ntext(x = 312, y = 825, adj = c(0.5, 0), family = \"Fiendish\", cex = 2,\r\n     label = \"Remember\\n\\nTo keep 2m apart\\n\\nin the seating area\")\r\n\r\n\r\n\r\nNext up we use emojifont to make some extra graphics featuring halloween stuff. In this case spiders, bats, and skulls, but emojifont::search_emoji(str = \"\") shows that are far more available. The original graphics had an annoying border top and left, image_crop() was used to erase these.\r\n\r\n\r\nspider <- image_graph(width = 300, height = 400, res = 50)\r\nggplot() + \r\n  geom_emoji(\"spider\", color='black') + \r\n  theme_void() + \r\n  theme(panel.background = element_rect(fill = \"#FFEE00\"))\r\nspider <- image_crop(spider, \"299x399+1+1\") %>% \r\n  image_scale(50)\r\n\r\nbat <- image_graph(width = 300, height = 400, res = 50)\r\nggplot() + \r\n  geom_emoji(\"bat\", color='black') + \r\n  theme_void() + \r\n  theme(panel.background = element_rect(fill = \"#FFEE00\"))\r\nbat <- image_crop(bat, \"299x399+1+1\") %>% \r\n  image_scale(50)\r\n\r\nscream <- image_graph(width = 300, height = 400, res = 50)\r\nggplot() + \r\n  geom_emoji(\"scream\", color='black') + \r\n  theme_void() + \r\n  theme(panel.background = element_rect(fill = \"#FFEE00\"))\r\nscream <- image_crop(scream, \"319x349+1+1\") %>% \r\n  image_scale(80) %>% \r\n  image_rotate(270)\r\n\r\nskull <- image_graph(width = 300, height = 400, res = 50)\r\nggplot() + \r\n  geom_emoji(\"skull\", color='black') + \r\n  theme_void() + \r\n  theme(panel.background = element_rect(fill = \"#FFFFFF\")) \r\nskull <- image_crop(skull, \"299x329+1+1\") %>% \r\n  image_scale(60)\r\n\r\nhead_bandage <- image_graph(width = 300, height = 400, res = 50)\r\nggplot() + \r\n  geom_emoji(\"face_with_head_bandage\", color='black') + \r\n  theme_void() + \r\n  theme(panel.background = element_rect(fill = \"#FFFFFF\")) \r\nhead_bandage <- image_crop(head_bandage, \"299x309+1+1\") %>% \r\n  image_scale(60)\r\n\r\n\r\n\r\nAnd finally to the complete poster, adding in our emojifont graphics. The exact position took a bit of trial and error.\r\n\r\n\r\ncorona %>% image_composite(spider) %>% \r\n  image_composite(bat, offset = \"+200\") %>% \r\n  image_composite(skull, offset = \"+148+272\") %>% \r\n  image_composite(head_bandage, offset = \"+420+275\") %>% \r\n  image_composite(scream, offset = \"+535+550\") %>%\r\n  image_resize(\"2000x1500\")\r\n\r\n\r\n\r\n\r\nHope you like it.\r\nHappy Halloween everyone, and stay safe.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-27-halloween-poster/images/covid-header.png",
    "last_modified": "2023-01-17T14:13:34+00:00",
    "input_file": {},
    "preview_width": 317,
    "preview_height": 127
  },
  {
    "path": "posts/2021-10-10-electoral-vote/",
    "title": "Electoral Vote",
    "description": "Tracking US politician's in the news.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-10-10",
    "categories": [
      "Politics",
      "Web Scraping"
    ],
    "contents": "\r\nElectoral-vote.com is one of my favourite websites. It gives a daily digest of news pertaining to US elections, usually getting around 11:00 GMT, syncing pretty well with my mid-morning coffee. It’s written by Andrew Tanenbaum (who wrote Minix, a pioneering Unix-like system), and Christopher Bates (an American historian based in UCLA). It combines facts and opinions an edgy sense of humour that makes it a compelling read.\r\nOf course, for the past few years, US President #45 has loomed large over such reporting, so I thought it’d be interesting to look back and see his presence, measured by how often he is name-checked on the site, has changed over time. Hence a little time series chart.\r\nAs well as tracking Trump, we also followed mentions of Clinton and Biden. I just counted how often the names appeared, so Trump could refer to him or his children, Clinton is mostly Hillary, but it could be her husband. And we focused on the bulk of the webpage, excluding headers and footers. Here’s how it was done:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nev_mentions <- function(url) {\r\n  w <- rvest::read_html(url)\r\n  pathway_data_html <- rvest::html_nodes(w, xml)\r\n  ev <- rvest::html_text(pathway_data_html)\r\n  z <- str_c(ev, collapse = T)\r\n  biden <- str_count(z, \"Biden\")\r\n  trump <- str_count(z, \"Trump\")\r\n  clinton <- str_count(z, \"Clinton\")\r\n  list(biden = biden,\r\n       trump = trump,\r\n       clinton = clinton,\r\n       length = str_count(z))\r\n}\r\n\r\nxml <-  \".top-box , .news-box li , p\" # the selector gadget is cool for figuring out which of the webpage are necessary\r\n\r\nev_time_period <- as_date(mdy(\"11-30-14\"):today())\r\n\r\nyear <- epiyear(ev_time_period)\r\nmonth <- months(ev_time_period, abbreviate = T)\r\nday <- format.Date(ev_time_period, \"%d\")\r\nurls <- glue::glue(\"https://www.electoral-vote.com/evp{year}/Pres/Maps/{month}{day}.html\")\r\nurl_senate <- glue::glue(\"https://www.electoral-vote.com/evp{year}/Senate/Maps/{month}{day}.html\")\r\n\r\nz <- z %>% mutate(quarters = quarters(date))\r\n\r\nmissing <- z %>%\r\n  filter(length < 200)\r\nmissing_year <- lubridate::epiyear(missing$date)\r\nmissing_month <- months(missing$date, abbreviate = T)\r\nmissing_day <- format.Date(missing$date, \"%d\")\r\nurls_senate <- glue::glue(\"https://www.electoral-vote.com/evp{missing_year}/Senate/Maps/{missing_month}{missing_day}.html\")\r\n\r\nz_missing <- urls_senate %>%\r\n  map_df(possibly(ev_mentions,\r\n                  list(biden = NA,\r\n                       trump = NA,\r\n                       clinton = NA))) %>%\r\n  bind_cols(date = missing$date)\r\n\r\nz <- urls %>%\r\n  map_df(possibly(ev_mentions,\r\n                  list(biden = NA,\r\n                       trump = NA,\r\n                       clinton = NA))) %>%\r\n  bind_cols(date = ev_time_period)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nwidth <- 21 # choice of rolling average number of days\r\n\r\nz1 <- z %>% \r\n  filter(length > 200) %>% \r\n  mutate(trump_width = zoo::rollapply(trump, width=width, mean, na.rm = TRUE, align = \"center\", fill = NA),\r\n         biden_width = zoo::rollapply(biden, width=width, mean, na.rm = TRUE, align = \"center\", fill = NA),\r\n         clinton_width = zoo::rollapply(clinton, width=width, mean, na.rm = TRUE, align = \"center\", fill = NA)) %>% \r\n  mutate(trump_width = ifelse(length < 200, NA, trump_width),\r\n         biden_width = ifelse(length < 200, NA, biden_width),\r\n         clinton_width = ifelse(length < 200, NA, clinton_width)) %>% \r\n  mutate(trump = ifelse(length < 200, NA, trump),\r\n         biden = ifelse(length < 200, NA, biden),\r\n         clinton = ifelse(length < 200, NA, clinton)) %>% \r\n  select(-length)\r\n\r\nz2 <- z1 %>% \r\n  pivot_longer(-c(date, trump, biden, clinton), \r\n               names_to = \"candidate_width\", \r\n               values_to = \"mentions_width\") %>% \r\n  select(-c(trump, clinton, biden))\r\n\r\nz3 <- z1 %>% \r\n  pivot_longer(-c(date, trump_width, biden_width, clinton_width), \r\n               names_to = \"candidate\", \r\n               values_to = \"mentions\") %>% \r\n  select(-c(trump_width, clinton_width, biden_width)) %>% \r\n  left_join(z2)\r\n\r\n\r\n\r\nNext we need some background bits and pieces for the plot, such as a set of date breaks I wanted to have vertical bands in the plot, a palette of colours, and a plot title. I thought I’d pick fonts that somehow tied in with the candidates: something official and neat for Clinton, familiar and friendly for Biden, and dark and foreboding for Trump.\r\n\r\n\r\ncolour_stripe <- c(\"grey20\", \"grey30\", \"grey40\", \"grey50\")\r\ndate_range_matrix <- matrix(as.numeric(seq.Date(from = min(z$date), \r\n                                                to = max(z$date), by = \"quarter\")), \r\n                            ncol = 2, byrow = TRUE)\r\ndate_range_df <- tibble::tibble(start = zoo::as.Date.numeric(date_range_matrix[,1]), \r\n                                end = zoo::as.Date.numeric(date_range_matrix[, 2]))\r\ndate_breaks <- c(date_range_df$start, date_range_df$end)\r\ncandidate_colours <- c(\"#bfd200\", \"#046c9a\", \"#972D15\")\r\nplot_title <- glue::glue(\"Daily EV Mentions for <i style= 'font-family: forum; color:{candidate_colours[1]}; font-size: 60px;'> Clinton<\/i>, \",\r\n                         \"<i style='font-family: Kalam; color:{candidate_colours[2]}; font-size: 60px;'>Biden<\/i>, \",\r\n                         \"and <b><i style = 'font-family:drakon; color:{candidate_colours[3]}; font-size: 60px;'>Trump<\/i><\/b>\")\r\n\r\n\r\n\r\nAnd now for the plot itself:\r\n\r\n\r\n\r\n\r\n\r\nz3 %>% ggplot(aes(date, mentions, colour = candidate)) + \r\n  geom_line(aes(y=mentions_width, colour = candidate_width), show.legend = F, size = 1.2) +\r\n  scale_color_manual(values = c(clinton_width = candidate_colours[1], \r\n                                biden_width = candidate_colours[2], \r\n                                trump_width = candidate_colours[3])) +\r\n  scale_x_yearqtr(breaks = date_breaks, \r\n                  lab = format(date_breaks, \r\n                               ifelse(month(date_breaks) == 08, \"%b\\n%Y\", \"%b\"))) +\r\n  geom_point(data = z1, aes(y = trump), \r\n             colour = candidate_colours[3], \r\n             size = 0.5, \r\n             alpha = 0.5) +\r\n  theme_minimal() +\r\n  geom_rect(data = date_range_df, \r\n            aes(xmin = start, xmax = end,\r\n                ymin = -Inf, ymax = Inf), \r\n            inherit.aes = FALSE,\r\n            alpha = 0.4,\r\n            fill = \"grey90\") +\r\n  coord_cartesian(ylim = c(0, 80)) +\r\n  labs(title = plot_title,\r\n       x = \"\",\r\n       y = \"\",\r\n       caption = glue::glue(\"lines are {width} day rolling means, <i><span style = 'color:{candidate_colours[3]};'> &bull; <\/span><\/i>'s are Trump daily counts\")) +\r\n  theme(plot.title = element_markdown(size = 50, hjust = 0.5),\r\n        text = element_text(family = \"Ink Free\", size = 32),\r\n        plot.caption = element_markdown(),\r\n        axis.title.x = element_markdown()) +\r\n  annotate(\"text\", x=as.Date(\"2016-05-06\"), y=70, \r\n           label = \"Election\\n2016\",\r\n           col = \"black\",\r\n           hjust = \"center\",\r\n           fontface = 2,\r\n           family = \"Ink Free\",\r\n           size = 8,\r\n           lineheight = 0.5) + \r\n  annotate(geom = \"curve\", \r\n           x = as.Date(\"2016-06-26\"), y = 70,\r\n           xend = as.Date(\"2016-11-06\"), yend = 55,\r\n           curvature = -0.3, arrow = arrow(length = unit(2, \"mm\"))) +\r\n  annotate(\"text\", x=as.Date(\"2021-04-06\"), y=70, \r\n           label = \"Election\\n2020\",\r\n           col = \"black\",\r\n           hjust = \"center\",\r\n           fontface = 2,\r\n           family = \"Ink Free\",\r\n           size = 8,\r\n           lineheight = 0.5) + \r\n  annotate(geom = \"curve\", \r\n           x = as.Date(\"2021-02-26\"), y = 70,\r\n           xend = as.Date(\"2020-11-06\"), yend = 65,\r\n           curvature = 0.3, arrow = arrow(length = unit(2, \"mm\"))) +\r\n  annotate(\"text\", x=as.Date(\"2018-07-15\"), y = 70, \r\n           label = \"137\\nmentions\\non February 26\\nCohen's Testimony\",\r\n           col = \"black\",\r\n           hjust = \"center\",\r\n           fontface = 2,\r\n           family = \"Ink Free\",\r\n           size = 8,\r\n           lineheight = 0.5) + \r\n  annotate(geom = \"curve\", \r\n           x = as.Date(\"2018-11-01\"), y = 70,\r\n           xend = as.Date(\"2019-02-26\"), yend = 80,\r\n           curvature = 0.4, arrow = arrow(length = unit(2, \"mm\"))) +\r\n  annotate(\"label\", x=as.Date(\"2020-08-15\"), y = 8, \r\n           label = \"Trump Free\\nDay\",\r\n           col = \"black\",\r\n           hjust = \"center\",\r\n           fontface = \"bold\",\r\n           family = \"Nanum\",\r\n           size = 8,\r\n           lineheight = 0.5) + \r\n  annotate(geom = \"curve\", \r\n           x = as.Date(\"2021-01-01\"), y = 8,\r\n           xend = as.Date(\"2021-03-16\"), yend = 2, size = 2,\r\n           curvature = -0.4, arrow = arrow(length = unit(4, \"mm\")))\r\n\r\n\r\n\r\nNote how Trump dominated even before the election of 2016, even though he was very much the outsider for that race.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-10-10-electoral-vote/electoral-vote_files/figure-html5/plot-1.png",
    "last_modified": "2023-01-17T14:13:34+00:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1152
  },
  {
    "path": "posts/2021-09-24-university-rankings/",
    "title": "University Rankings",
    "description": "Seeing the world through QS university rankings.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-09-24",
    "categories": [
      "Academia",
      "R",
      "Maps"
    ],
    "contents": "\r\nQuacquarelli Symonds produces a list of the top ranked international universities every year. I thought it’d be interesting to see how this looks across the globe, not so much with a map, but with a silhouette showing the positions of the top colleges.\r\nQS publish their data, but it’s a little awkward to scrape them, so I referred to this kaggle dataset with the figures for 2020.\r\n\r\n\r\n\r\n\r\n\r\nkaggle_dataset <- read_csv(\"data/2020-QS-World-University-Rankings.csv\",\r\n              skip = 1,\r\n              locale = readr::locale(encoding = \"latin1\"))\r\n\r\nuniversity_rankings <- tibble(university = kaggle_dataset$...3, \r\n                              rank = 1:nrow(kaggle_dataset))\r\n\r\n\r\n\r\nNote that we had to change the encoding for the csv file (readr::locale(encoding = \"latin1\")) to capture international lettering.\r\nNext, to find out where these universties are. I used both ggmap and tidygeocoder, finding an address isn’t so easy when all you have is the universities name, and then combined the two to fill out missed values. Only 6 of 1024 universities went unlocated.\r\n\r\n\r\n# geos0 <- tidygeocoder::geo(address = z$university, method = \"osm\")\r\n# geos1 <- ggmap::geocode(location = z$university, output = \"latlona\")\r\n# geos2 <- bind_cols(geos0, geos1)\r\n# geos <- geos2 %>%\r\n#   mutate(lat = ifelse(is.na(lat...2), lat...5, lat...2),\r\n#          long = ifelse(is.na(long), lon, long)) %>%\r\n#   select(university = address...1, address = address...6, long, lat)\r\n\r\ngeos <- readRDS(\"data/geos\")\r\nuniversity_rankings <- university_rankings %>% left_join(geos)\r\n\r\n\r\n\r\nAnd now the map. No fancy simple features or projections, just locations specified by longitude and latitude. But that’s enough to see the outline of the world’s landmasses.\r\nLot’s of universities in Europe, East Coast America, and across the South and East of Asia. Also, note the crescent of colleges following the Southern outline of the Himalayas. And the near total absence of most of Africa.\r\nI’d hope this picture will look very different in years to come, with a much wider spread of colleges across the globe.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-24-university-rankings/university-rankings_files/figure-html5/map-1.png",
    "last_modified": "2023-01-17T14:13:34+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-29-donegals-architecture/",
    "title": "Donegal's Architectural Heritage",
    "description": "Having a look at some of the historical buildings in Donegal, thanks to data from the National Inventory of Architectural Heritage. Also a chance to check out crosstalk and leafpop. Note; images will take a while to appear",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-08-29",
    "categories": [
      "Maps",
      "Donegal"
    ],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nDonegal Historic Buildings\r\nFilter by Year and Type\r\n\r\nYear\r\n\r\n\r\nType\r\n\r\n\r\n\r\n\r\nbridge\r\n\r\n\r\n\r\n\r\nchurch/chapel\r\n\r\n\r\n\r\n\r\n\r\n\r\ncountry house\r\n\r\n\r\n\r\n\r\nfarm house\r\n\r\n\r\n\r\n\r\n\r\n\r\ngate lodge\r\n\r\n\r\n\r\n\r\ngates/railings/walls\r\n\r\n\r\n\r\n\r\n\r\n\r\nhouse\r\n\r\n\r\n\r\n\r\noutbuilding\r\n\r\n\r\n\r\n\r\n\r\n\r\nwater pump\r\n\r\n\r\n\r\n\r\nOther\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-29-donegals-architecture/img/oakpark.jpg",
    "last_modified": "2023-01-17T14:13:33+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-24-horn-head-in-3d/",
    "title": "Horn Head in 3D",
    "description": "Horn Head gets the Rayshader Treatment.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-08-24",
    "categories": [
      "Maps",
      "Donegal"
    ],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe Donegal coast is pretty spectacular, no where more so than the beaches and cliffs around Sheephaven Bay. This post follows my attempt to produce a three dimensional model of part of this, namely Horn Head, using the rayshader package.\r\nTo produce the model, we’ll need:\r\nan elevation matrix\r\nan image of the peninsula to overlay on this\r\nLet’s deal with the later first.\r\nSatellite Image of Horn Head from Sentinel-2\r\nAccessing Sentinel-2 satellite images follows from the work of Luigi Rancetti, and a workshop from Ewa Grabska. I wanted an image from this Summer that we just spent up in Donegal, and on a day with low cloud coverage. The code to do this is shown below:\r\n\r\n\r\nlibrary(getSpatialData)\r\nlibrary(sen2r)\r\n\r\nset_aoi() # set to area around Dunfanaghy, -8, 55.2\r\ntime_range =  c(\"2021-06-01\", \"2021-08-15\") #set time range\r\nplatform = \"Sentinel-2\" #choose platform\r\nlogin_CopHub(username = login$user, password = login$pass) #login to Esa SCI-HUB\r\n\r\nsentinel_records <- getSentinel_records(time_range, \"Sentinel-2\") %>% \r\n  filter(cloudcov < 20) %>% \r\n  select(record_id, date_acquisition, cloudcov)\r\n\r\n\r\n\r\nThis gives the following table of images:\r\n\r\nTile\r\nOrbit\r\nDate\r\nCloud\r\n29UNB\r\n32052\r\n2021-08-11\r\n12.68%\r\n29UNB\r\n32052\r\n2021-08-11\r\n9.37%\r\n29UNB\r\n22786\r\n2021-07-17\r\n1.06%\r\n29UNB\r\n31623\r\n2021-07-12\r\n18.58%\r\n29UNB\r\n22357\r\n2021-06-17\r\n15.78%\r\n\r\nThe image from July 17th is clearly the best for clouds so we’ll work with this. It has a record_id of best_id = S2B_MSIL1C_20210717T115359_N0301_R023_T29UNB_20210717T140640.\r\nTo download all 734.33 MB of this, we use the following code: s2_download(best_id, outdir = \"data\").\r\nThe images we need are buried deep in the ensuing download directory under data/S2B_MSIL2A_20210717T115359_N0301_R023_T29UNB_20210717T144509.SAFE/GRANULE/L2A_T29UNB_A022786_20210717T115402/IMG_DATA/R10m/ (look under the .SAFE directory then GRANULE, then look for IMG_DATA. The R10m means 10m resolution).\r\nTo construct the image we’ll use:\r\n\r\n\r\nbase_image_file_10m <- \"data/S2B_MSIL2A_20210717T115359_N0301_R023_T29UNB_20210717T144509.SAFE/GRANULE/L2A_T29UNB_A022786_20210717T115402/IMG_DATA/R10m/T29UNB_20210717T115359_\"\r\nb02_image_file_10m <- paste0(base_image_file_10m, \"B02_10m.jp2\")\r\nb03_image_file_10m <- paste0(base_image_file_10m, \"B03_10m.jp2\")\r\nb04_image_file_10m <- paste0(base_image_file_10m, \"B04_10m.jp2\")\r\nhorn_head_10m <- raster::stack(b02_image_file_10m, \r\n                       b03_image_file_10m, \r\n                       b04_image_file_10m)\r\next = c(560000, 570000, 6113000, 6123000) # horn-head\r\nhorn_head_10m_crop <- crop(horn_head_10m, ext)\r\nplotRGB(horn_head_10m_crop, r = 3, g = 2, b = 1, stretch = \"lin\")\r\n\r\n\r\n\r\nGiving:\r\n\r\n\r\n\r\nWith a crs and extent of:\r\n\r\n[1] \"+proj=utm +zone=29 +datum=WGS84 +units=m +no_defs\"\r\nclass      : Extent \r\nxmin       : 560000 \r\nxmax       : 570000 \r\nymin       : 6113000 \r\nymax       : 6123000 \r\n\r\nElevation Data for Horn Head\r\nFor this part, we’re leaning on the work of Derek Watkins and the Shuttle Radar Topography Mission. From the website dwtkns.com/srtm30m/ you can zoom in on the relevant tiles (in this case N55W009 and N55W008) and download elevation data.\r\nThese tiles are then loaded into R, they are merged, and then cropped to the area of Horn Head. Note that we have to play around with projections because the elevation data comes in latlong format, but the sentinel data is utm. Then we have to change the elevation data in to a matrix. We finish by clipping the previous image data to the same extent as this elevation data.\r\n\r\n\r\nN55W009 <- raster(\"data/SRTM/N55W009.hgt\")\r\nN55W008 <- raster(\"data/SRTM/N55W008.hgt\")\r\nN55 <- raster::merge(N55W009, N55W008)\r\n\r\nbottom_left = c(y=-8.058146434999939, x=55.17420873750392)\r\ntop_right   = c(y=-7.9232205897354686, x=55.233572425130)\r\nextent_latlong = sp::SpatialPoints(rbind(bottom_left, top_right), proj4string=sp::CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84\"))\r\nextent_utm = sp::spTransform(extent_latlong, raster::crs(horn_head_10m))\r\n\r\nN55_crop <- crop(N55, extent_latlong)\r\nN55_crop_utm <- projectRaster(N55_crop, crs = crs(horn_head_10m), method = \"bilinear\")\r\nN55_crop_utm_matrix <- rayshader::raster_to_matrix(N55_crop_utm)\r\n\r\nhorn_head_10m_crop <- crop(horn_head_10m, extent(N55_crop_utm))\r\n\r\n\r\n\r\nThis looks like this:\r\n\r\nPutting it Together with rayshader\r\nFor this part, we refer to Tyler Morgan-Wall. The Horn Head Image produced by plotRGB is saved to disk as a png and then opened up as an image.\r\n\r\n\r\nimg <- png::readPNG(\"img/horn-head-2021.png\")\r\n\r\nplot_3d(img, N55_crop_utm_matrix, windowsize = c(1000,1000), zscale = 5, shadowdepth = -50,\r\n        zoom=0.5, phi=45,theta=-45,fov=70, background = \"#F2E1D0\", shadowcolor = \"#523E2B\")\r\n\r\nrender_snapshot(title_text = \"Horn Head, Donegal | Imagery: Sentinel-2 | DEM: 30m SRTM\",\r\n                title_bar_color = \"#1f5214\", title_color = \"white\", title_bar_alpha = 1, asp=2)\r\n\r\n\r\n\r\nThe final snapshot looks something like this. Note the difficulty in layering over the steep cliffs on the North of the peninsula, I guess that’s asking too much of Sentinel to capture them:\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-24-horn-head-in-3d/img/three-D-model.png",
    "last_modified": "2023-01-17T14:13:33+00:00",
    "input_file": {},
    "preview_width": 535,
    "preview_height": 360
  },
  {
    "path": "posts/2021-06-12-world-ocean-day/",
    "title": "World Ocean Day",
    "description": "A map of the North Atlantic off the Donegal Coast, with pop-ups detailing various species observed at different locations.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-06-08",
    "categories": [
      "Maps",
      "Donegal"
    ],
    "contents": "\r\nThis is World Ocean Day, and seeing as this time of year always finds us on the Donegal Coast, looking out over the North Atlantic, it seemed like a good time for a blog post.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThis place is rife with wildlife, with a strong emphasis on bio-diversity, so I wanted that to feature in the post as well. Thus the idea became to produce a map showing observations of different species in the area.\r\n\r\n\r\nThe Observations\r\nFor this, there is no better place than the Global Biodiversity Information Facility. The r package rgbif links directly to this, with the function occ_search doing the heavy lifting. The function can be spatially limited to a bounding box, we picked an area of about 20,000 km2. off the North and West Donegal coast. This is shown in the first code chunk below:\r\n\r\n\r\n\r\n\r\n\r\nbounds <- c('55.4045 -10.4181', \r\n            '54.2264 -10.4205',\r\n            '54.2433 -8.4768',\r\n            '54.5967 -8.1391',\r\n            '54.6907 -8.7753',\r\n            '55.1823 -8.1036',\r\n            '55.2986 -7.0557',\r\n            '55.4798 -7.3945')\r\n\r\nwkt <- glue::glue(\r\n  \"POLYGON(({bounds[1]}, \r\n  {bounds[2]}, \r\n  {bounds[3]}, \r\n  {bounds[4]}, \r\n  {bounds[5]}, \r\n  {bounds[6]}, \r\n  {bounds[7]}, \r\n  {bounds[8]}, \r\n  {bounds[1]}))\"\r\n  )\r\n\r\nspecies_observations <- occ_search(geometry = wkt)$data %>% \r\n  select(-key) %>% \r\n  distinct() %>% \r\n  mutate(sciname = str_remove(acceptedScientificName, \"\\\\([^()]*\\\\)\"),\r\n         sciname = str_remove(sciname, \"[^A-Z][A-Z][^q]*\"),\r\n         sciname = str_trim(sciname),\r\n         date = strftime(eventDate, format = \"%d %B %Y\"))\r\n\r\n\r\n\r\n\r\n\r\n\r\nSpecies\r\n\r\n\r\nlongitude\r\n\r\n\r\nlatitude\r\n\r\n\r\ndate\r\n\r\n\r\nPuffinus bailloni (Bonaparte, 1857)\r\n\r\n\r\n-7.522000\r\n\r\n\r\n55.33000\r\n\r\n\r\n17 February 2020\r\n\r\n\r\nPhaethon lepturus Daudin, 1802\r\n\r\n\r\n-7.522000\r\n\r\n\r\n55.33000\r\n\r\n\r\n17 February 2020\r\n\r\n\r\nPuffinus pacificus (Gmelin, 1789)\r\n\r\n\r\n-7.630240\r\n\r\n\r\n55.47330\r\n\r\n\r\n04 December 2017\r\n\r\n\r\nCoryphaena hippurus Linnaeus, 1758\r\n\r\n\r\n-9.850140\r\n\r\n\r\n54.40014\r\n\r\n\r\n25 April 2013\r\n\r\n\r\nAluterus monoceros (Linnaeus, 1758)\r\n\r\n\r\n-9.850140\r\n\r\n\r\n54.40014\r\n\r\n\r\n25 April 2013\r\n\r\n\r\nCanthidermis maculata (Bloch, 1786)\r\n\r\n\r\n-9.850140\r\n\r\n\r\n54.40014\r\n\r\n\r\n25 April 2013\r\n\r\n\r\nCarcharhinus falciformis (Müller & Henle, 1839)\r\n\r\n\r\n-9.850140\r\n\r\n\r\n54.40014\r\n\r\n\r\n25 April 2013\r\n\r\n\r\nElagatis bipinnulata (Quoy & Gaimard, 1825)\r\n\r\n\r\n-9.850140\r\n\r\n\r\n54.40014\r\n\r\n\r\n25 April 2013\r\n\r\n\r\nAcanthocybium solandri (Cuvier, 1832)\r\n\r\n\r\n-9.850140\r\n\r\n\r\n54.40014\r\n\r\n\r\n25 April 2013\r\n\r\n\r\nCoryphaena hippurus Linnaeus, 1758\r\n\r\n\r\n-9.266806\r\n\r\n\r\n54.61680\r\n\r\n\r\n28 April 2013\r\n\r\n\r\nSphyraena barracuda (Edwards, 1771)\r\n\r\n\r\n-9.266806\r\n\r\n\r\n54.61680\r\n\r\n\r\n28 April 2013\r\n\r\n\r\nElagatis bipinnulata (Quoy & Gaimard, 1825)\r\n\r\n\r\n-9.266806\r\n\r\n\r\n54.61680\r\n\r\n\r\n28 April 2013\r\n\r\n\r\nAcanthocybium solandri (Cuvier, 1832)\r\n\r\n\r\n-9.266806\r\n\r\n\r\n54.61680\r\n\r\n\r\n28 April 2013\r\n\r\n\r\nUraspis secunda (Poey, 1860)\r\n\r\n\r\n-9.216805\r\n\r\n\r\n54.31680\r\n\r\n\r\n26 April 2012\r\n\r\n\r\nCanthidermis maculata (Bloch, 1786)\r\n\r\n\r\n-9.216805\r\n\r\n\r\n54.31680\r\n\r\n\r\n26 April 2012\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.556530\r\n\r\n\r\n54.39290\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.500850\r\n\r\n\r\n54.40074\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.540150\r\n\r\n\r\n54.37861\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.541860\r\n\r\n\r\n54.37899\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.554230\r\n\r\n\r\n54.39298\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-10.260960\r\n\r\n\r\n54.32670\r\n\r\n\r\n03 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-10.379740\r\n\r\n\r\n54.41201\r\n\r\n\r\n03 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-10.377480\r\n\r\n\r\n54.42170\r\n\r\n\r\n03 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-10.283770\r\n\r\n\r\n54.40229\r\n\r\n\r\n03 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-10.386200\r\n\r\n\r\n54.44261\r\n\r\n\r\n03 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.627830\r\n\r\n\r\n54.42259\r\n\r\n\r\n05 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.645710\r\n\r\n\r\n54.40990\r\n\r\n\r\n05 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.782440\r\n\r\n\r\n54.41589\r\n\r\n\r\n05 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.785720\r\n\r\n\r\n54.41016\r\n\r\n\r\n05 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.602510\r\n\r\n\r\n54.39647\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.621440\r\n\r\n\r\n54.39744\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.624210\r\n\r\n\r\n54.40755\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.643130\r\n\r\n\r\n54.39529\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.703760\r\n\r\n\r\n54.32497\r\n\r\n\r\n07 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.638080\r\n\r\n\r\n54.39200\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.075270\r\n\r\n\r\n54.42868\r\n\r\n\r\n06 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.088690\r\n\r\n\r\n54.43781\r\n\r\n\r\n06 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.588270\r\n\r\n\r\n54.39122\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-8.567130\r\n\r\n\r\n54.38071\r\n\r\n\r\n08 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.232940\r\n\r\n\r\n54.45467\r\n\r\n\r\n06 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.288500\r\n\r\n\r\n54.45316\r\n\r\n\r\n06 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.303040\r\n\r\n\r\n54.47156\r\n\r\n\r\n06 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.310510\r\n\r\n\r\n54.45819\r\n\r\n\r\n06 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.595710\r\n\r\n\r\n54.43569\r\n\r\n\r\n05 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.297480\r\n\r\n\r\n54.47985\r\n\r\n\r\n06 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.589590\r\n\r\n\r\n54.43498\r\n\r\n\r\n05 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.092280\r\n\r\n\r\n54.42542\r\n\r\n\r\n06 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.133930\r\n\r\n\r\n54.44247\r\n\r\n\r\n06 May 2011\r\n\r\n\r\nCaretta caretta (Linnaeus, 1758)\r\n\r\n\r\n-9.147140\r\n\r\n\r\n54.43363\r\n\r\n\r\n06 May 2011\r\n\r\n\r\nLaridae\r\n\r\n\r\n-7.272760\r\n\r\n\r\n55.36494\r\n\r\n\r\n29 March 2010\r\n\r\n\r\nLaridae\r\n\r\n\r\n-7.283010\r\n\r\n\r\n55.34349\r\n\r\n\r\n29 March 2010\r\n\r\n\r\nLaridae\r\n\r\n\r\n-7.279960\r\n\r\n\r\n55.34931\r\n\r\n\r\n29 March 2010\r\n\r\n\r\nLaridae\r\n\r\n\r\n-7.285820\r\n\r\n\r\n55.33851\r\n\r\n\r\n29 March 2010\r\n\r\n\r\nLaridae\r\n\r\n\r\n-7.290610\r\n\r\n\r\n55.33140\r\n\r\n\r\n29 March 2010\r\n\r\n\r\nLaridae\r\n\r\n\r\n-7.271480\r\n\r\n\r\n55.36757\r\n\r\n\r\n29 March 2010\r\n\r\n\r\nLaridae\r\n\r\n\r\n-7.279140\r\n\r\n\r\n55.35100\r\n\r\n\r\n29 March 2010\r\n\r\n\r\nLaridae\r\n\r\n\r\n-7.256800\r\n\r\n\r\n55.39631\r\n\r\n\r\n01 April 2010\r\n\r\n\r\nLaridae\r\n\r\n\r\n-7.281310\r\n\r\n\r\n55.35083\r\n\r\n\r\n01 April 2010\r\n\r\n\r\nFregata Lacepede, 1799\r\n\r\n\r\n-7.287990\r\n\r\n\r\n55.33856\r\n\r\n\r\n01 April 2010\r\n\r\n\r\nEretmochelys imbricata (Linnaeus, 1766)\r\n\r\n\r\n-8.500000\r\n\r\n\r\n54.50000\r\n\r\n\r\n06 August 2010\r\n\r\n\r\nChelonia mydas (Linnaeus, 1758)\r\n\r\n\r\n-7.783472\r\n\r\n\r\n55.31680\r\n\r\n\r\n09 April 2007\r\n\r\n\r\nSphyraena barracuda (Edwards, 1771)\r\n\r\n\r\n-7.783472\r\n\r\n\r\n55.31680\r\n\r\n\r\n09 April 2007\r\n\r\n\r\nCarcharhinus falciformis (Müller & Henle, 1839)\r\n\r\n\r\n-7.683472\r\n\r\n\r\n55.28347\r\n\r\n\r\n09 April 2007\r\n\r\n\r\nKajikia audax (Philippi, 1887)\r\n\r\n\r\n-7.683472\r\n\r\n\r\n55.28347\r\n\r\n\r\n09 April 2007\r\n\r\n\r\nCarcharhinus falciformis (Müller & Henle, 1839)\r\n\r\n\r\n-7.466805\r\n\r\n\r\n55.31680\r\n\r\n\r\n09 April 2007\r\n\r\n\r\nBalaenoptera physalus (Linnaeus, 1758)\r\n\r\n\r\n-7.466805\r\n\r\n\r\n55.31680\r\n\r\n\r\n09 April 2007\r\n\r\n\r\nParacaesio xanthura (Bleeker, 1869)\r\n\r\n\r\n-9.500000\r\n\r\n\r\n55.00000\r\n\r\n\r\n01 October 1987\r\n\r\n\r\nConger cinereus Rüppell, 1830\r\n\r\n\r\n-9.500000\r\n\r\n\r\n55.00000\r\n\r\n\r\n01 October 1987\r\n\r\n\r\nRudgea viburnoides (Cham.) Benth.\r\n\r\n\r\n-9.583333\r\n\r\n\r\n54.91667\r\n\r\n\r\n19 April 1983\r\n\r\n\r\nThunnus albacares (Bonnaterre, 1788)\r\n\r\n\r\n-7.218047\r\n\r\n\r\n55.28505\r\n\r\n\r\n23 January 1974\r\n\r\n\r\nThunnus obesus (Lowe, 1839)\r\n\r\n\r\n-7.218047\r\n\r\n\r\n55.28505\r\n\r\n\r\n23 January 1974\r\n\r\n\r\nDasyatis Rafinesque, 1810\r\n\r\n\r\n-7.218047\r\n\r\n\r\n55.28505\r\n\r\n\r\n23 January 1974\r\n\r\n\r\nAlopias vulpinus (Bonnaterre, 1788)\r\n\r\n\r\n-7.218047\r\n\r\n\r\n55.28505\r\n\r\n\r\n23 January 1974\r\n\r\n\r\nAlepisaurus ferox Lowe, 1833\r\n\r\n\r\n-7.218047\r\n\r\n\r\n55.28505\r\n\r\n\r\n23 January 1974\r\n\r\n\r\nThunnus albacares (Bonnaterre, 1788)\r\n\r\n\r\n-7.318617\r\n\r\n\r\n55.35211\r\n\r\n\r\n25 January 1974\r\n\r\n\r\nThunnus obesus (Lowe, 1839)\r\n\r\n\r\n-7.318617\r\n\r\n\r\n55.35211\r\n\r\n\r\n25 January 1974\r\n\r\n\r\nDasyatis Rafinesque, 1810\r\n\r\n\r\n-7.318617\r\n\r\n\r\n55.35211\r\n\r\n\r\n25 January 1974\r\n\r\n\r\nCarcharhinus longimanus (Poey, 1861)\r\n\r\n\r\n-7.318617\r\n\r\n\r\n55.35211\r\n\r\n\r\n25 January 1974\r\n\r\n\r\nThunnus obesus (Lowe, 1839)\r\n\r\n\r\n-7.452750\r\n\r\n\r\n55.40240\r\n\r\n\r\n26 January 1974\r\n\r\n\r\nPrionace glauca (Linnaeus, 1758)\r\n\r\n\r\n-7.452750\r\n\r\n\r\n55.40240\r\n\r\n\r\n26 January 1974\r\n\r\n\r\nAlepisaurus ferox Lowe, 1833\r\n\r\n\r\n-7.452750\r\n\r\n\r\n55.40240\r\n\r\n\r\n26 January 1974\r\n\r\n\r\nCyclothone Goode & Bean, 1883\r\n\r\n\r\n-8.700000\r\n\r\n\r\n55.11670\r\n\r\n\r\n11 February 1964\r\n\r\n\r\nGonostomatidae\r\n\r\n\r\n-8.700000\r\n\r\n\r\n55.11670\r\n\r\n\r\n11 February 1964\r\n\r\n\r\nNemesis lamna lamna\r\n\r\n\r\n-8.700000\r\n\r\n\r\n55.12000\r\n\r\n\r\n17 February 1964\r\n\r\n\r\nDinemoura latifolia (Steenstrup & Lütken, 1861)\r\n\r\n\r\n-8.700000\r\n\r\n\r\n55.12000\r\n\r\n\r\n17 February 1964\r\n\r\n\r\nAmphipoda\r\n\r\n\r\n-8.700000\r\n\r\n\r\n55.12000\r\n\r\n\r\n17 February 1964\r\n\r\n\r\nPhyseter macrocephalus Linnaeus, 1758\r\n\r\n\r\n-10.324640\r\n\r\n\r\n54.36641\r\n\r\n\r\n01 January 1913\r\n\r\n\r\nPhyseter macrocephalus Linnaeus, 1758\r\n\r\n\r\n-8.369530\r\n\r\n\r\n55.25718\r\n\r\n\r\n01 January 1913\r\n\r\n\r\nDagetichthys Stauch & Blanc, 1964\r\n\r\n\r\n-10.000000\r\n\r\n\r\n55.00000\r\n\r\n\r\n01 January 1770\r\n\r\n\r\nPseudocycnus appendiculatus Heller, 1865\r\n\r\n\r\n-8.920000\r\n\r\n\r\n55.13000\r\n\r\n\r\nNA\r\n\r\n\r\nGloiopotes watsoni Kirtisinghe, 1934\r\n\r\n\r\n-8.920000\r\n\r\n\r\n55.13000\r\n\r\n\r\nNA\r\n\r\n\r\nPandarus satyrus Dana, 1849\r\n\r\n\r\n-8.920000\r\n\r\n\r\n55.13000\r\n\r\n\r\nNA\r\n\r\n\r\n\r\n\r\nThe scientific species names are a little obscure, I’d like to map them to more familiar names. That’s where the package taxize comes in. It can map the scientific names of species to their common names via the function sci2comm. But first we need to remove the information in parentheses for each observation, e.g (Gmelin, 1789), that gives the naturalist and date of the species’ classification.\r\n\r\n\r\nspecies_observations <- species_observations %>% \r\n  select(Species = acceptedScientificName,\r\n         longitude = decimalLatitude, # note how rgbif mixes up long and lat\r\n         latitude = decimalLongitude, # note how rgbif mixes up long and lat\r\n         phylum,\r\n         order,\r\n         family,\r\n         genus,\r\n         eventDate) %>% \r\n  distinct() %>% \r\n  mutate(sciname = str_remove(Species, \"\\\\([^()]*\\\\)\"),\r\n         sciname = str_remove(sciname, \"[^A-Z][A-Z][^q]*\"),\r\n         sciname = str_trim(sciname))\r\n\r\n\r\n\r\nNow we can run the mapping to the species’ common name.\r\n\r\n\r\n# species_observations$commonname <- sci2comm(species_observations$sciname)\r\n# species_observations <- species_observations %>% \r\n#   mutate(commonname = unlist(commonname))\r\nspecies_observations <- readRDS(file = \"data/gbif\")\r\n\r\n\r\n\r\nLet’s follow this up by making some neater labels for the map’s pop-ups:\r\n\r\n\r\nspecies_observations <- species_observations %>% \r\n  select(commonname, everything()) %>% \r\n  mutate(commonname = ifelse(commonname == \"character(0)\", sciname, commonname),\r\n         commonname = str_to_title(commonname),\r\n         genus = ifelse(is.na(genus), \"\", glue::glue(\"{genus}<br>\")),\r\n         family = ifelse(is.na(family), \"\", glue::glue(\"{family}<br>\")))\r\n\r\n\r\n\r\nAnd now, let’s draw our map, colouring the points by the year of observation. Seeing as we’re talking about the ocean here, I thought a bathyscape map would be best, hence the addProviderTiles(providers$Esri.OceanBasemap) line.\r\n\r\n\r\npal <- colorNumeric(\"Reds\", domain=(species_observations$year))\r\n\r\nspecies_observations %>% \r\n  leaflet() %>% \r\n  addProviderTiles(providers$Esri.OceanBasemap) %>%\r\n  setView(lng=-8.5, lat=54.8, zoom=7.2) %>% \r\n  addCircleMarkers(lng = ~ decimalLatitude, # note how rgbif mixes up long and lat\r\n             lat = ~ decimalLongitude,# note how rgbif mixes up long and lat\r\n             popup = ~paste(\"<b>\", {commonname}, \"<\/b><br>\",  \r\n                            phylum, \"<br>\",\r\n                            order, \"<br>\",\r\n                            family,\r\n                            genus,\r\n                            strftime(species_observations$eventDate, \r\n                                     format = \"%d %B %Y\")),\r\n             color = ~pal(year),\r\n             radius = 5)\r\n\r\n\r\n\r\n\r\nSee if you can find the two whales, one off Malin and the second off Belmullet, and the Portugeese sole from the 18th century.\r\nNow, time for some watersports….\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-06-12-world-ocean-day/images/sheephaven.png",
    "last_modified": "2023-01-17T14:13:33+00:00",
    "input_file": {},
    "preview_width": 1208,
    "preview_height": 673
  },
  {
    "path": "posts/2021-05-06-solvay-conference/",
    "title": "Solvay Conference",
    "description": "This is what it looks like to change the world.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-05-06",
    "categories": [
      "Images",
      "R"
    ],
    "contents": "\r\nWe posted previously about FACE++ and the article from Theresa Kuntzler showing how to integrate this into R. I couldn’t resist taking this for a spin one more time, on this occasion using it to examine one of the most famous conference photographs of them all, the one from the 5th Solvay Conference of 1927.\r\nHere is the photograph, it features a veritable who’s-who of physicists and chemists from a century ago. As a physicist, seeing the people that gave rise to so many brilliant theories and techniques always brings a thrill.\r\n.\r\nWe ran the image through FACE++ to generate a tibble, called faces, of names and emotions. Here it is:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nfaces %>% \r\n  select(scientist, anger:surprise) %>% \r\n  kable(\"html\") %>%\r\n  kable_styling() %>%\r\n   scroll_box(\r\n    height = \"200px\",\r\n    box_css = \"border: 1px solid #ddd; padding: 5px; \",\r\n    fixed_thead = TRUE\r\n  )\r\n\r\n\r\n\r\n\r\nscientist\r\n\r\n\r\nanger\r\n\r\n\r\ndisgust\r\n\r\n\r\nfear\r\n\r\n\r\nhappiness\r\n\r\n\r\nneutral\r\n\r\n\r\nsadness\r\n\r\n\r\nsurprise\r\n\r\n\r\ncompton\r\n\r\n\r\n0.000\r\n\r\n\r\n0.104\r\n\r\n\r\n0.000\r\n\r\n\r\n0.004\r\n\r\n\r\n99.891\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\nlangevin\r\n\r\n\r\n27.209\r\n\r\n\r\n4.816\r\n\r\n\r\n0.295\r\n\r\n\r\n0.804\r\n\r\n\r\n46.320\r\n\r\n\r\n9.604\r\n\r\n\r\n10.952\r\n\r\n\r\ndebroglie\r\n\r\n\r\n0.151\r\n\r\n\r\n0.092\r\n\r\n\r\n11.442\r\n\r\n\r\n0.092\r\n\r\n\r\n27.725\r\n\r\n\r\n38.821\r\n\r\n\r\n21.677\r\n\r\n\r\nguye\r\n\r\n\r\n0.147\r\n\r\n\r\n0.147\r\n\r\n\r\n4.745\r\n\r\n\r\n1.453\r\n\r\n\r\n63.292\r\n\r\n\r\n30.049\r\n\r\n\r\n0.167\r\n\r\n\r\nborn\r\n\r\n\r\n0.005\r\n\r\n\r\n0.006\r\n\r\n\r\n0.005\r\n\r\n\r\n4.869\r\n\r\n\r\n94.881\r\n\r\n\r\n0.173\r\n\r\n\r\n0.059\r\n\r\n\r\ncurie\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.017\r\n\r\n\r\n99.982\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\nkramers\r\n\r\n\r\n0.026\r\n\r\n\r\n0.014\r\n\r\n\r\n3.978\r\n\r\n\r\n0.024\r\n\r\n\r\n95.930\r\n\r\n\r\n0.014\r\n\r\n\r\n0.014\r\n\r\n\r\nlorentz\r\n\r\n\r\n97.117\r\n\r\n\r\n0.009\r\n\r\n\r\n0.009\r\n\r\n\r\n0.009\r\n\r\n\r\n0.027\r\n\r\n\r\n2.678\r\n\r\n\r\n0.151\r\n\r\n\r\ndirac\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.002\r\n\r\n\r\n0.020\r\n\r\n\r\n98.913\r\n\r\n\r\n0.000\r\n\r\n\r\n1.064\r\n\r\n\r\neinstein\r\n\r\n\r\n0.001\r\n\r\n\r\n0.001\r\n\r\n\r\n0.001\r\n\r\n\r\n0.001\r\n\r\n\r\n99.922\r\n\r\n\r\n0.001\r\n\r\n\r\n0.075\r\n\r\n\r\ndebye\r\n\r\n\r\n67.131\r\n\r\n\r\n0.087\r\n\r\n\r\n0.182\r\n\r\n\r\n0.009\r\n\r\n\r\n32.355\r\n\r\n\r\n0.036\r\n\r\n\r\n0.199\r\n\r\n\r\nlangmuir\r\n\r\n\r\n0.008\r\n\r\n\r\n0.008\r\n\r\n\r\n0.097\r\n\r\n\r\n0.105\r\n\r\n\r\n99.332\r\n\r\n\r\n0.008\r\n\r\n\r\n0.442\r\n\r\n\r\nknudsen\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n100.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\nplanck\r\n\r\n\r\n89.112\r\n\r\n\r\n0.014\r\n\r\n\r\n0.014\r\n\r\n\r\n0.148\r\n\r\n\r\n10.300\r\n\r\n\r\n0.026\r\n\r\n\r\n0.385\r\n\r\n\r\nbragg\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n100.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\nheisenberg\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n99.996\r\n\r\n\r\n0.002\r\n\r\n\r\n0.000\r\n\r\n\r\n0.002\r\n\r\n\r\nfowler\r\n\r\n\r\n0.002\r\n\r\n\r\n0.000\r\n\r\n\r\n0.004\r\n\r\n\r\n0.001\r\n\r\n\r\n99.948\r\n\r\n\r\n0.044\r\n\r\n\r\n0.000\r\n\r\n\r\nbrillioun\r\n\r\n\r\n0.001\r\n\r\n\r\n0.000\r\n\r\n\r\n0.006\r\n\r\n\r\n0.000\r\n\r\n\r\n99.991\r\n\r\n\r\n0.000\r\n\r\n\r\n0.002\r\n\r\n\r\npiccard\r\n\r\n\r\n30.095\r\n\r\n\r\n0.018\r\n\r\n\r\n0.056\r\n\r\n\r\n0.018\r\n\r\n\r\n69.260\r\n\r\n\r\n0.202\r\n\r\n\r\n0.349\r\n\r\n\r\nhenriot\r\n\r\n\r\n0.002\r\n\r\n\r\n0.002\r\n\r\n\r\n0.027\r\n\r\n\r\n0.005\r\n\r\n\r\n99.938\r\n\r\n\r\n0.014\r\n\r\n\r\n0.012\r\n\r\n\r\nehrenfest\r\n\r\n\r\n7.918\r\n\r\n\r\n0.081\r\n\r\n\r\n0.336\r\n\r\n\r\n0.949\r\n\r\n\r\n90.683\r\n\r\n\r\n0.017\r\n\r\n\r\n0.017\r\n\r\n\r\nherzen\r\n\r\n\r\n0.077\r\n\r\n\r\n0.865\r\n\r\n\r\n0.077\r\n\r\n\r\n32.781\r\n\r\n\r\n66.045\r\n\r\n\r\n0.077\r\n\r\n\r\n0.077\r\n\r\n\r\ndedonder\r\n\r\n\r\n0.012\r\n\r\n\r\n0.012\r\n\r\n\r\n0.053\r\n\r\n\r\n12.241\r\n\r\n\r\n86.798\r\n\r\n\r\n0.012\r\n\r\n\r\n0.872\r\n\r\n\r\nschroddinger\r\n\r\n\r\n0.013\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.001\r\n\r\n\r\n99.985\r\n\r\n\r\n0.000\r\n\r\n\r\n0.001\r\n\r\n\r\nvershaffelt\r\n\r\n\r\n3.014\r\n\r\n\r\n0.069\r\n\r\n\r\n2.943\r\n\r\n\r\n0.761\r\n\r\n\r\n90.604\r\n\r\n\r\n0.069\r\n\r\n\r\n2.540\r\n\r\n\r\npauli\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n99.838\r\n\r\n\r\n0.001\r\n\r\n\r\n0.161\r\n\r\n\r\nwilson\r\n\r\n\r\n1.850\r\n\r\n\r\n10.580\r\n\r\n\r\n0.160\r\n\r\n\r\n22.290\r\n\r\n\r\n63.527\r\n\r\n\r\n0.160\r\n\r\n\r\n1.433\r\n\r\n\r\nbohr\r\n\r\n\r\n0.239\r\n\r\n\r\n0.049\r\n\r\n\r\n0.063\r\n\r\n\r\n0.049\r\n\r\n\r\n69.860\r\n\r\n\r\n29.690\r\n\r\n\r\n0.049\r\n\r\n\r\nrichardson\r\n\r\n\r\n0.001\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n0.000\r\n\r\n\r\n99.994\r\n\r\n\r\n0.000\r\n\r\n\r\n0.004\r\n\r\n\r\n\r\n\r\nAs you can see, the over-riding emotion comes out as being neutral. The modern vogue of having a sea of smiling faces for your conference photograph wasn’t obviously a thing back in 1927. But there is enough variation to warrant taking a closer look. Below we show boxplots of all seven emotions. The percentage % is transformed to a logit scale to highlight variations. From the outliers we can see signs of sadness and disgust and that someone is particularly happy.\r\n\r\n\r\nmy_colours <- paletteer::paletteer_d(\"yarrr::eternal\", n = 7)\r\n\r\nbox <- faces %>% \r\n  select(scientist, anger:surprise) %>% \r\n  pivot_longer(-scientist, names_to = \"emotion\", values_to = \"percentage\") %>% \r\n  ggplot(aes(emotion, percentage %>% gtools::logit(max = 100))) +\r\n  geom_boxplot(aes(fill = emotion), show.legend = F) +\r\n  scale_fill_manual(values = my_colours) +\r\n  labs(x = \"\", y = \"Percentage %\") +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.text.y = element_text(color = my_colours)) +\r\n  coord_flip()\r\n\r\nbox_dat <- ggplot_build(box)$data[[1]]\r\n\r\nbox + \r\n  geom_segment(data = box_dat, \r\n               aes(x=xmin, xend=xmax,\r\n                   y=middle, yend=middle), \r\n               colour=\"grey80\", size=1)\r\n\r\n\r\n\r\n\r\n\r\nHeisenberg - Happiness 100 %\r\n\r\nLorentz - Anger 97.1 %\r\n\r\nde Broglie - Sadness 38.8 %\r\n\r\n\r\nIt seems that FACE++ is remarkably good at picking up pretty subtle facial expressions, even in a photograph of 29 people from 94 years ago. Not bad.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-06-solvay-conference/solvay-conference_files/figure-html5/boxplot-1.png",
    "last_modified": "2023-01-17T14:13:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-03-derry-girls/",
    "title": "Derry Girls",
    "description": "Using FACE++ to read emotions on images of faces.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-05-03",
    "categories": [
      "Images",
      "R"
    ],
    "contents": "\r\nTheresa Kuentzler wrote a nice post, linking R to the Face++ API. So I thought I’d give it a go too.\r\nFirst, we need to choose an image with faces, and for this crucial decision we formed a focus group in the family here. The basic FACE++ limits us to five faces per image so this ruled out Modern Family, Friends, and Oceans 8. Images from Poirot, Stargate, and Line of Duty featured faces that were pretty stoic so not too interesting emotion-wise. But then we looked at the magnificent Derry Girls television series and knew we had found our mark. It tells the story of a bunch of schoolchildren in 1990’s Derry, set against the back-drop of the Troubles and the Peace Process, with a great sound track and an irreverent sense of humour.\r\nThe image chosen looks like this:\r\n\r\n\r\nmypaths <- \"images/derry-girls.jpg\"\r\nderry_girls <- magick::image_read(mypaths)\r\nplot(derry_girls)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nFace++ needs registration and authorisation keys, the post from Theresa mentioned above discusses how to do this. I wasn’t keen to have my key on github, so it’s created here from a file outside the repo.\r\n\r\n\r\nmyauth <- readRDS(\"../../../myauth_faceplusplus\")\r\n\r\n\r\n\r\nThe function below is the workhorse of this post, again largely created based on the code of Theresa. Note the block of fromJSON() statements in the middle; one of Hadley’s Rules of Programming is that if you repeat code more that twice it should become its own function but in this case it seemed to be clearer to let these statements stand on their own.\r\n\r\n\r\nface_plus_plus <- function(fullpath) {\r\n  face <- httr::RETRY(\"POST\", \"https://api-us.faceplusplus.com/facepp/v3/detect\",\r\n                      body = list(api_key  = myauth$api_key,\r\n                                  api_secret = myauth$api_secret,\r\n                                  image_file = upload_file(fullpath),\r\n                                  return_landmark = 0,\r\n                                  return_attributes = \"emotion,gender\"),\r\n                      times = 2, \r\n                      encode = \"multipart\") %>% \r\n    as.character\r\n  \r\n  anger <- fromJSON(face)$faces$attributes$emotion$anger\r\n  disgust <- fromJSON(face)$faces$attributes$emotion$disgust\r\n  fear <- fromJSON(face)$faces$attributes$emotion$fear\r\n  happiness <- fromJSON(face)$faces$attributes$emotion$happiness\r\n  neutral <- fromJSON(face)$faces$attributes$emotion$neutral\r\n  sadness <- fromJSON(face)$faces$attributes$emotion$sadness\r\n  surprise <- fromJSON(face)$faces$attributes$emotion$surprise\r\n  gender <- fromJSON(face)$faces$attributes$gender\r\n  top <- fromJSON(face)$faces$face_rectangle$top\r\n  left <- fromJSON(face)$faces$face_rectangle$left\r\n  tibble(anger, disgust, fear, happiness, neutral,\r\n         sadness, surprise, top, left, \r\n         gender = gender$value, image = fullpath)\r\n}\r\n\r\n\r\n\r\nAnd now we can run our function. We have to manually code the character names, and I decided to recode the position so that the origin was in the bottom left corner to make it synchronise better with the plot to come.\r\n\r\n\r\nderry <- map_df(mypaths, face_plus_plus) %>% \r\n  arrange(left) %>% \r\n  mutate(name = c(\"Michelle\", \"James\", \"Erin\", \"Orla\", \"Claire\"),\r\n         x = left,\r\n         y = height - top) %>% \r\n  select(-c(image, top, left))\r\n\r\nderry %>% gt()\r\n\r\n\r\n\r\nanger\r\n      disgust\r\n      fear\r\n      happiness\r\n      neutral\r\n      sadness\r\n      surprise\r\n      gender\r\n      name\r\n      x\r\n      y\r\n    0.046\r\n0.046\r\n0.066\r\n65.650\r\n26.834\r\n6.803\r\n0.556\r\nFemale\r\nMichelle\r\n92\r\n3212.374\r\n1.132\r\n0.301\r\n0.079\r\n12.066\r\n7.675\r\n76.374\r\nMale\r\nJames\r\n186\r\n3320.006\r\n0.217\r\n0.006\r\n0.006\r\n21.087\r\n0.006\r\n78.670\r\nFemale\r\nErin\r\n315\r\n3260.000\r\n0.009\r\n0.009\r\n99.926\r\n0.002\r\n0.000\r\n0.053\r\nFemale\r\nOrla\r\n438\r\n3290.002\r\n0.018\r\n1.847\r\n0.002\r\n0.002\r\n0.002\r\n98.125\r\nFemale\r\nClaire\r\n532\r\n259\r\n\r\nNext up, we made a separate data table to generate labels for our plot below. It discarded emotions that are less than 10% for each character, and it builds in some html to format the labels. The font, Amiri was the best match I could find to the text in the school logo, the colour, #004400, lines up with the uniform colour.\r\n\r\n\r\nemotions <- derry %>% \r\n  select(-c(gender, x, y)) %>% \r\n  pivot_longer(cols = -c(name), \r\n               names_to = \"emotion\", \r\n               values_to = \"percentage\") %>% \r\n  dplyr::filter(percentage > 10) %>% \r\n  mutate(percentage = round(percentage, 1)) %>% \r\n  unite(\"emotion\", emotion:percentage, sep = \": \") %>% \r\n  mutate(emotion = glue::glue(\"{emotion}%\")) %>% \r\n  group_by(name) %>% \r\n  summarise(emotion = paste(emotion, collapse = \"<br>\")) %>% \r\n  ungroup() %>% \r\n  mutate(name1 = glue::glue(\"<b>{name}<\/b>\")) %>% \r\n  unite(\"emotion\", name1:emotion, sep = \"<br>\") %>% \r\n  mutate(emotion = glue::glue(\"<p style = 'color:#004400; font-size:28px;  font-family:Amiri';>{emotion}<\/p>\"))\r\n\r\n\r\n\r\nPutting this together, using ggplot() with background_image() from the ggpubr package gives:\r\n\r\n\r\nderry %>% \r\n  left_join(emotions) %>% \r\n  ggplot(aes(x, y)) +\r\n  coord_cartesian(xlim = c(0, width), \r\n                  ylim = c(0, height)) +\r\n  background_image(derry_girls) +\r\n  ggtext::geom_richtext(aes(x = x + ifelse(x > 100, \r\n                                           sign(x-centre[1])*50 + 50, \r\n                                           sign(x-centre[1])*50 + 10),\r\n                            y = y + ifelse(x>400, \r\n                                           sign(y-centre[2])*65,\r\n                                           sign(y-centre[2])*(-120)),\r\n                            label = emotion)) +\r\n  theme_void()\r\n\r\n\r\n\r\n\r\nThe label positions were a bit hit-and-miss, but I wanted to use the face positions as discovered by FACE++ rather than manually code the positions.\r\nSeems like FACE++ captured the emotions expressed on these faces pretty well, now if only it could produce a script as sharp as that of Lisa McGee….\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-03-derry-girls/derry-girls_files/figure-html5/emotions-1.png",
    "last_modified": "2023-03-22T11:06:26+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-08-schizophrenia-genes/",
    "title": "Schizophrenia Genes",
    "description": "A karyogram of the genes featured in GWAS of schizophrenia.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-04-26",
    "categories": [
      "Bioinformatics"
    ],
    "contents": "\r\nWe’ve been working with the genetics of schizophrenia, looking at how the expression of genes that play a role in the condition vary across brain regions and neurodevelopment stages. Now, call me old-fashioned, but I like to know how things are laid out, so in this case where these genes are situated. Time for a plot.\r\nThe genes in question come from a GWAS study of the PGC. The images are build based on the ggbio and karyoploteR packages from bioconductor.\r\nFirst, let’s load up the required packages:\r\n\r\n\r\n\r\nNext, we get a list of the genes from the PGC. These are taken from one of the supplementary tables from the 2018 paper mentioned above. We only care about the ones that also feature in the Allen Brain Atlas, so we filter for those.\r\n\r\n\r\ngenes <- genes::pardinas() %>% \r\n  filter(genes %in% abagenes$hgnc_symbol) %>% \r\n  pull(genes)\r\n\r\n\r\n\r\nFrom our gene list we build a data frame of gene information using Human build 38 (grch38 from the annotables package). And then turn his into an appropriate GRanges object (thank you GenomicRanges).\r\n\r\n\r\ngene_table <- grch38 %>% \r\n  dplyr::filter(symbol %in% genes) %>% \r\n  dplyr::filter(chr %in% c(1:22, \"X\", \"Y\")) %>% \r\n  dplyr::mutate(strand = ifelse(strand == 1, \"+\", \"-\"))\r\n\r\ngene_ranges <- makeGRangesFromDataFrame(gene_table, keep.extra.columns = T)\r\nseqlevelsStyle(gene_ranges) <- \"UCSC\"\r\n\r\n\r\n\r\nNow for the plot. The plotKaryotype() function does a lqyout of 24 chromosomes with their cytobands, the kpPlotMarkers() adds in the gene labels. The label colours depend on strand; red for + and green for -. I wasn’t too successful avoiding label overlaps, sorry about that.\r\n\r\n\r\nkp <- plotKaryotype(genome=\"hg38\")\r\nkpPlotMarkers(kp, \r\n              data=gene_ranges, \r\n              labels=gene_ranges$symbol, \r\n              label.color = ifelse(gene_table$strand == \"+\", \"darkred\", \"olivedrab4\"),\r\n              text.orientation = \"horizontal\",\r\n              label.dist = 0.0001,\r\n              r1=0.5, cex=0.6, adjust.label.position = T)\r\n\r\n\r\n\r\n\r\nOur genes are pretty spread out, with no particular pattern of course. But I just feel I know them a little better now.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-08-schizophrenia-genes/schizophrenia-genes_files/figure-html5/karyogram-1.png",
    "last_modified": "2023-01-17T14:13:33+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-08-meteorites/",
    "title": "Meteorites....",
    "description": "...and where to find them.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-04-25",
    "categories": [
      "Physics"
    ],
    "contents": "\r\n\r\n\r\n\r\nThis analysis was inspired by a set of talks by Mike Brown of Caltech on small bodies in the Solar System (these are available as part of his course on The Science of the Solar System on Coursera). He speaks with passion about these fascinating objects and how they tell an intriguing story about the formation of our Solar System. Most of the science you’ll meet within this blog comes either from this course or else an excellent book, Physics and Chemistry of the Solar System by John Lewis.\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe data come from NASA (where else) and contains information on over 35,000 meteorites up to end 2015. In terms of munging, the data is:\r\nrescaled for meteorite mass in kilogrammes\r\nlocations (in longitude/latitude) are attributed to their continents (using a function from Andy South)\r\nsome meteors were missing geographical coordinates, an attempt was made to find these and fill them in. This worked for the Nunatak and Yamato fields, both in Antarctica\r\ncleaned to remove remaining incomplete records (6463 of them)\r\nmeteorite classes are grouped together. For example, Iron IIIAB, Iron IAB, Iron IC, …. are all just called Irons.\r\nmeteorites are further grouped into Chondrite, Achondrites, and Irons (with some subclasses of achondrite based on metal content as there are so many achondrites)\r\nwe delete meteorites with a mass of 0kg. There are a handful of these and they mess up our analysis somewhat and are obviously physically impossible\r\nthere are a large number (6164) where the (longitude, latitude) is given as (0\\(^{\\circ}\\), 0\\(^{\\circ}\\)). These get dumped in the Atlantic Ocean, just off the coast of Ghana (see the bright red dot in the middle of the map). The name of the meteorite should help locate them, at some point I’ll do something about that. Lots of them are in Antarctica.\r\nGeographical Spread\r\nFirst, let’s look at where meteorites have been found. The map below shows our meteors. The circle sizes correspond to meteor mass (though you have to zoom in a bit to appreciate the different sizes), the colour indicates the year of discovery. You can click on circles to get these details, as well as the meteor name. To avoid map navigation being tediously slow, only the heaviest meteorites are initially shown, but by clicking the top right legend you can include more.\r\n\r\n\r\n\r\nMap Analysis\r\nThe most striking feature of the map is the range in sizes of meteorites, from ones like the giant Hoba weighing in at tens of tonnes to more common, gram-sized meteorites.\r\nNext, we see areas with lots of red, where meteorites have been found more recently: throughout the Sahara, in Antarctica, and across the South central plain of Australia. These are locations in which bizarre looking rocks really stand out in places where they have no right to be. Antarctica has the further advantage that meteorite finds will gather at the foot of glaciers. Surprisingly, there isn’t that much in Greenland, maybe an untapped resource here.\r\nZooming in on North America, we notice the abundance of kilogramme-sized meteorites discovered in the mid-West throughout the 20th century.\r\nFor older meteorite discoveries, we look to Western Europe, the Eastern seaboard of the United States, the Indian subcontinent, and Japan.\r\nIn short, meteorites have been found where, a) there are lots of people, or b) they stand out.\r\nMass Distribution\r\nBelow we show a portion of a histogram of meteorite masses. It shows a predictable ~exponential decline. Note also, anomolously large numbers of meteorites with masses of exactly 1kg, 2kg….(indicated by ticks). My guess is that these were older samples with less rigorous mass measurements.\r\n\r\n\r\n\r\nBy Continent\r\nLet’s look at the spread of meteorite mass across different continents. This is shown in the figure below.\r\n\r\n\r\n\r\nNot much to see here. North America is the place to go for larger meteorites, but note the plethora of small meteorites from Antarctica. In fact, by far the greatest number of meteorites come from Antarctica (23368 of them out of a total number of 39018), something that’s hard to appreciate from the map above because they are frequently ascribed the same latitude, longitude and so their pointers sit on top of each other.\r\nFalls versus Finds\r\nThere are two categories of meteorites; those that we happen to come across by chance (finds), and those meteors that we see in descent, track, and discover based on their trajectory (falls). Is there a difference in mass? Let’s see.\r\n\r\n\r\n\r\nWhat we observe is that smaller meteorites tend to be finds. It’s difficult to track meteors, especially in the final, dark, part of their descent when the meteor velocity drops below 3m/s and the visible trail disappears. Unless they are large enough, or unless the terrain is suitable, they won’t be uncovered.\r\nAre We Running Out of Meteorites to Find?\r\nIf so we should see a drop off in meteorite mass over time. So let’s plot meteorite mass as a function of year of discovery. It’s shown below (in the plot we also distinguish between different types of meteorite by colour, more on that later):\r\n\r\n\r\n\r\nThere appears to be lots of structure in the plot. First off, something dramatic happened from the mid 1970’s on. We begin to see years with enormous numbers of small meteorite discovery. On investigation, these are due to years in which there are meteorite campaigns in Antarctica. Most notable is the Yamato campaign of 1979 that yielded 2935 meteorites with a combined mass of 301.14 kg. Then there was the Asuka campaign of 1988 with 1262 meteorites with a combined mass of 246.18 kg.\r\nWe also see some horizontal stripping in this plot. This is due to rounding off measurements of meteorite mass as mentioned above.\r\nThe colours in the plot describe the nature of the meteorite (see below for details). Note the yellow stripes corresponding to very low iron chondrites in 1997 and 1999. These are from the Queen Alexandra Range in Antarctica campaigns. Most of these are very low or low iron condrites, suggesting that they come from the same proginator meteoroid (though Queen Alexandra Range 94281 is a fascinating Lunar basaltic rock).\r\nAs to the original question, have we found all the big ones? It would appear not. Witness, for the example the 3 tonne Al Haggounia meteorite discovered in 2006 in Western Sahara or the equally massive Xifu meteorite from 2004 in China.\r\nTypes of Meteorite\r\nAt a very hand-waving level, meteorites can be divided into three types:\r\nChondrites. These are moderate sized lumps of rock that were formed in the early days of the Solar System and have remained pretty much as-is until their encounter with the Earth. They would have spent most of their lives in the asteroid belt, a disturbance sending them into an Earth crossing orbit. The Chrondrite name refers to the Chondrules that make them up. These are millimetre sized silicate spheres embedded in a carbon matrix.\r\nIrons. These come from the core of a small planetoid that formed early on in the history of the Solar System. It was hot, and large, enough to melt and differentiate, with the metals being dragged into the core. It then got whacked by something that made it disintegrate. The resulting fragments drifted through space until they fell to Earth. Because they are so easy to find, these are common meteorites, and the heaviest. Their structure and composition tell us a great deal about their parent body. Typically these were 20km to 500km in radius (think Vesta).\r\nAchondrites. The fragmented bodies that gave us irons from their core also give us rocks from their mantle. Because they melted they no longer have the millimetre-sized stone beads, so we call them achondrites.\r\nLet’s see how common the different types are, and how their masses stack up.\r\n\r\n\r\n\r\nThe four classes of chondrite seem to have similar mass distributions, something that can be investigated by an ANOVA test. The result is a p-value of 0.09. Looks like all the chondrites do indeed have similar masses. To have a closer look, a Tukey Test comes up with the following table\r\n\r\ncontrast\r\nnull.value\r\nestimate\r\nconf.low\r\nconf.high\r\nadj.p.value\r\nchondrite high iron-chondrite\r\n0\r\n-0.49\r\n-2.32\r\n1.34\r\n0.90\r\nchondrite low iron-chondrite\r\n0\r\n-0.21\r\n-2.06\r\n1.64\r\n0.99\r\nchondrite vlow iron-chondrite\r\n0\r\n-1.42\r\n-3.47\r\n0.63\r\n0.28\r\nchondrite low iron-chondrite high iron\r\n0\r\n0.27\r\n-0.61\r\n1.16\r\n0.86\r\nchondrite vlow iron-chondrite high iron\r\n0\r\n-0.93\r\n-2.18\r\n0.31\r\n0.22\r\nchondrite vlow iron-chondrite low iron\r\n0\r\n-1.21\r\n-2.48\r\n0.07\r\n0.07\r\n\r\nFrom this we see that all types of chondrites have pretty similar masses.\r\nNow let’s see how they spread out across the globe. This is shown in the map below. There are lots of low iron (13434) and high iron (16330) chondrites, so the map will be slow to update and navigate when these are selected. The distribution of most types look similar, with the exception of the irons. My guess is that this is a finding bias, there are terrains like the American mid-West which favour finding a lump of iron but where non-iron meteorites won’t stand out so much. And it just goes to show that there must be umpteen meteorites out there waiting to be picked up.\r\n\r\n\r\n\r\nFinally, Back to those Yamato Meteorites\r\nDid they all come from the same source? Was it one incoming meteoroid that shattered into thousands of peices? If so, they should all have similar compositions. We looked at this, taking the proportional representation of each class in the Yamato field and compared to the overall meteorite population. For good measure, we did this for the Asuka field as well. The results are shown in the table below for the most common classes in the yamato field:\r\n\r\nMeteorite Category\r\nYamato\r\nall\r\nAsuka\r\nQAR\r\nH4\r\n0.274\r\n0.102\r\n0.269\r\n0.006\r\nL6\r\n0.148\r\n0.199\r\n0.156\r\n0.098\r\nH5\r\n0.146\r\n0.164\r\n0.160\r\n0.124\r\nH6\r\n0.097\r\n0.107\r\n0.079\r\n0.079\r\nLL6\r\n0.038\r\n0.044\r\n0.030\r\n0.027\r\nH4/5\r\n0.033\r\n0.010\r\nNA\r\nNA\r\nL4\r\n0.030\r\n0.025\r\n0.025\r\n0.006\r\nLL\r\n0.030\r\n0.006\r\n0.002\r\nNA\r\nE3\r\n0.028\r\n0.005\r\nNA\r\nNA\r\nL5\r\n0.020\r\n0.089\r\n0.024\r\n0.193\r\nH3\r\n0.016\r\n0.009\r\n0.023\r\nNA\r\nL3\r\n0.016\r\n0.008\r\n0.034\r\nNA\r\nCM2\r\n0.013\r\n0.009\r\n0.004\r\n0.007\r\nDiogenite\r\n0.010\r\n0.005\r\n0.009\r\n0.001\r\nLL-melt breccia\r\n0.007\r\n0.001\r\nNA\r\nNA\r\nLL4\r\n0.006\r\n0.005\r\n0.015\r\n0.004\r\nEucrite\r\n0.005\r\n0.003\r\n0.005\r\nNA\r\nEucrite-pmict\r\n0.005\r\n0.004\r\n0.002\r\nNA\r\nLL5\r\n0.005\r\n0.057\r\n0.002\r\n0.427\r\nLL5/6\r\n0.005\r\n0.001\r\nNA\r\n0.000\r\n\r\nYamato and Asuka seem to track each other pretty closely. They do differ significantly from the overall population of meteorites, especially for H4, a high iron chondrite with abundant chondrules. Our guess is that this is simply a collection bias. H4’s are tough to distinguish from regular rocks, but on the Antarctic ice sheet that’s not a big problem.\r\nOn the other hand, the Queen Alexandra Range (QAR) meteorites show a significant departure from both the overall meteorite population and also their Antarctic brethren. They have considerably more low iron chondrites (L5 and LL5). Maybe in this case, they are fragments from the same progenitor meteoroid.\r\nTo Wrap Up\r\nThis is a cursory look at the locations and natures of meteorites. They are fascinating. One thing not discussed is the science that unfolds from meteorite examination. The ability to collect samples from the early Solar System, from diverse sectors of our Solar System, and study them in laboratories here on Earth has revealed rich information about the formation and inner workings of our home. It is an active and fruitful area of research. The staggering number of meteorites being uncovered nowadays mean new insights are inevitable.\r\nFinally. The best place to find meteorites; Antarctica. Barring that, the Sahara. But meteorites are ubiquitous, the best way to find them is to know what you’re looking for by checking out some pictures or by visiting your local museum of natural history.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-08-meteorites/meteorites_files/figure-html5/mass_year-1.png",
    "last_modified": "2023-01-17T14:13:32+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-08-tennis-sets/",
    "title": "Tennis Sets",
    "description": "My tennis coach thinks that tennis matches frequently feature a close first set followed by a blow-out in the second.",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netlify.app"
      }
    ],
    "date": "2021-04-25",
    "categories": [
      "Fun",
      "Sport"
    ],
    "contents": "\r\nThe Concept\r\nOur tennis coach has a theory. As well as being a teacher and player he’s also an avid follower of the game. And he reckons that an inordinate amount of competitive, two-set tennis matches finish with a closely matched first set followed by a relative blow out in the second. 7-5, 6-2 just about covers it.\r\nJimmy’s thinking is that a player who loses the first set, however close it was, will try to change things up to rescue the match. Every now and again this tactic works, and a player wins when they were otherwise destined to lose. However, mostly it just makes things worse and leads to the kind of scores that Jimmy predicts. But what odds, losing 7-5 6-2 is no different than losing 7-5 7-5. Is he right, or is this another example of confirmation bias rearing his ugly head? Let’s see what the data says.\r\nThe Data\r\nThere is a raft of tennis statistics out there, we referred to the excellent repo of Jeff Sackmann which you can clone here. It features a series of csv files with scores and figures from the ATP tour going back into the 60’s. It has over 50 columns, suggesting perhaps a whole series of tennis related data blogs….. We focused on data just from 1990 on, after all tennis has changed dramatically over the years and we don’t want to confound our problem.\r\n\r\n\r\n\r\nThe data needs a little work before we get going on analysis. We replace tie-break games, which normally include the score in the tie-break, with a clean 7-6 or 6-7. The original data-set has the overall score in a single column, we split that into separate columns, one for each set, and we add a column for year. Also, we only care about two set games (after all, this is the basis of Jimmy’s Theorem) and we need to get rid of games where one player retired injured, we do this by weeding out uncommon scorelines.\r\n\r\n\r\n#reformat tie-break set scores from 7-6(4) to just 7-6\r\natp$score <- gsub('\\\\(..\\\\)|\\\\(.\\\\)','', atp$score)\r\n\r\n#split the overall score into three set scores\r\natp <- atp %>% \r\n  separate(score, into = c(\"Set1\", \"Set2\", \"Set3\"), sep=\" \", fill=\"right\")\r\n\r\n#adding a year column to the dataframe\r\natp <- atp %>%\r\n  mutate(year = lubridate::year(ymd(tourney_date)))\r\n\r\n#pick out just the 2 set games\r\ntwo_sets = atp %>% \r\n  filter(is.na(Set3)) \r\n\r\n#weed out matches with retirements\r\nscores <-  names(table(two_sets$Set1)[table(two_sets$Set1)/length(two_sets$Set1)>0.005])\r\ntwo_sets <- two_sets %>% \r\n  filter(Set1 %in% scores, Set2 %in% scores)\r\n\r\n#show a little table to see what the frame looks like\r\ntwo_sets\r\n\r\n\r\n# A tibble: 45,616 × 5\r\n   Set1  Set2  Set3  tourney_date  year\r\n   <chr> <chr> <chr>        <dbl> <dbl>\r\n 1 7-6   6-1   <NA>      19900305  1990\r\n 2 7-5   6-0   <NA>      19900305  1990\r\n 3 6-2   6-1   <NA>      19900305  1990\r\n 4 6-1   6-0   <NA>      19900305  1990\r\n 5 6-3   7-5   <NA>      19900305  1990\r\n 6 6-4   6-4   <NA>      19900305  1990\r\n 7 6-4   6-2   <NA>      19900305  1990\r\n 8 6-4   7-5   <NA>      19900305  1990\r\n 9 6-3   6-2   <NA>      19900305  1990\r\n10 6-2   7-6   <NA>      19900305  1990\r\n# … with 45,606 more rows\r\n\r\n#finally, reduce the number of columns in the dataset\r\ntwo_sets <- two_sets %>% \r\n  select(Set1, Set2, year)\r\n\r\n\r\n\r\nWe end up with about 46 thousand scores covering 28 years.\r\nInvestigation\r\nFirst up, let’s examine the frequency of scores across the two sets to see if there is anything of note.\r\n\r\n\r\n\r\nIt seems there is a definite difference here. Set Two had significantly higher numbers of unbalanced sets (6-0, 6-1, 6-2) while Set One has more competitive scores. Maybe Jimmy is on to something.\r\nOut of curiosity, note the large number of 6-4 scores in set 2. This is because, if you loose the first set, there’s a strong chance you serve first in the second. In that case a single break of serve in the body of the set will lead to a score of 6-4.\r\nHow about score combinations, the real reason we’re here. The figure below presents these in a tiled format, the deeper the green the more often we see this match result. By Jimmy’s hypothesis, we should be seeing darker greens in the bottom right corner. These are the matches where set one was close and set two more one-sided.\r\n\r\n\r\n\r\nHmm. There doesn’t seem to be much here. The bottom right corner and top left look pretty comparable. Unsurprisingly, the centre of the diagram is where all the action is, and 6-4 6-4 is the most popular score.\r\nTo unpick this and see if there are any subtle effects, anything I can tell Jimmy, it’s time to call on a \\(\\chi^2\\) test.\r\n\r\n\r\n\r\nNot surprisingly, the \\(\\chi^2\\) statistic is highly significant and the scores from set one and set two are not independent. These are after all, the same two players facing off. We finish with a \\(\\chi^2\\) p-value of 3^{-130}.\r\nThe real story might be in the residuals. These are shown in the tiled plot below. Red squares indicate score combinations that are unusually frequent. Blue squares, scores that we don’t see as often as we’d expect.\r\n\r\n\r\n\r\nThe most striking feature is the bright red square in the top right corner. There are more matches with two 7-6 tie-breaks than we have a right to expect. My guess is that these matches involve at least one player who relies heavily on his service. These matches tend to have few service breaks and so sets progress unerringly towards deciding tie-breaks.\r\nNotice, also, the horizontal band of banal colours when Set2 is 7-5. There are two reasons for this; this is an unusual set score so there are few occurrences in this band, and also 7-5 seems to be the “neutral” second set score that tells us little about what happened in the first set.\r\nBut again, we see nothing untoward happening in the bottom right corner. It looks like our theory doesn’t bear scrutiny after all.\r\nOne last thing to look at is the evolution of set scores over the years. This is really just because we have the data in front of us rather than because it pertains to the specific question at hand. The graph is shown in the figure below. Most set scores seem pretty steady over time. The one exception appears to be the gradual rise in the number of tie-breaks. Maybe this is indicative of more of those heavy servers that we talked about above.\r\n\r\n\r\n\r\nConclusions\r\nIt looks like I’ll be having a difficult conversation with our tennis coach next time I see him. We found no evidence for the Jimmy Effect, despite a convincing rationale why it should be there from a man who knows what he’s talking about. He’s fond of this theory and I’m loathe to tell him that the data doesn’t seem to back it up.\r\nOf course, we could broaden this investigation, look at games from the woman’s tour and also doubles games. We’d have to factor in a Bonferroni correction to any statistical tests. But we might find that tactics play a greater role in these games and that could be crucial in bringing in to play the effect we’ve been looking for.\r\nSome footnotes here. First of all, this account bears more than a passing resemblance to the post Super Bowl Squares from David Robinson. If you haven’t read it already, it’s well presented and a worthy source of inspiration. Secondly, I’m a big fan of criticism. Anything short of blatant denigration is welcome. I’m especially conscious of one part of the code where I hardwired a sequence rather than let the analysis discover it for itself.\r\nThank you for reading, and thank you in advance for comments and suggestions.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-08-tennis-sets/tennis-sets_files/figure-html5/bar_chart-1.png",
    "last_modified": "2023-01-17T14:13:33+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Eugene's Blog",
    "description": "Welcome to our new blog, Eugene's Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Eugene",
        "url": "https://fizzics.netliffy.app"
      }
    ],
    "date": "2021-04-25",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-01-17T14:13:35+00:00",
    "input_file": {}
  }
]
